<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>复方汤剂</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2022-06-30T09:06:30.415Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>Ran</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>初识redis</title>
    <link href="http://example.com/2022/06/28/Getting-to-know-redis/"/>
    <id>http://example.com/2022/06/28/Getting-to-know-redis/</id>
    <published>2022-06-28T10:28:51.000Z</published>
    <updated>2022-06-30T09:06:30.415Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><blockquote><p> 本文基本是参考了《Redis核心技术与实战》的基础篇。</p></blockquote><p>本文需要搞明白的问题有：</p><ul><li>基本架构：一个键值数据库包含什么？</li><li>数据结构：快速的Redis有哪些慢操作？</li><li>高性能IO模型：为什么单线程Redis能那么快？</li><li>AOF日志&amp;RDB快照：宕机了，Redis如何避免数据丢失，如何实现数据恢复？</li><li>数据同步：主从库如何实现数据一致？</li><li>哨兵机制&amp;哨兵集群：主库挂了，如何不间断服务？哨兵挂了，主从库怎么切？</li><li>切片集群：数据增多了，是该加内存还是加实例？</li></ul><span id="more"></span><h1 id="基本架构：一个键值数据库包含什么？"><a href="#基本架构：一个键值数据库包含什么？" class="headerlink" title="基本架构：一个键值数据库包含什么？"></a>基本架构：一个键值数据库包含什么？</h1><p>Redis是一个键值数据库。</p><p>以下以一个简单的数值数据库SimpleKV为例，从架构方面来介绍设计一个键值数据库应当考虑哪些问题。</p><h2 id="存什么数据-amp-做什么操作"><a href="#存什么数据-amp-做什么操作" class="headerlink" title="存什么数据 &amp; 做什么操作"></a>存什么数据 &amp; 做什么操作</h2><h3 id="存什么数据"><a href="#存什么数据" class="headerlink" title="存什么数据"></a>存什么数据</h3><p>对于键值数据库而言，基本的数据模型是key-value模型。不同键值数据库支持的key类型一般差异不大，而value类型则有较大差别。我们在对键值数据库进行选型时，一个重要的考虑因素是<strong>它支持的value类型</strong>。例如，Memcached支持的value类型仅为String类型，而Redis支持的value类型包括了String、哈希表、列表、集合等。</p><p>从使用的角度来说，不同value类型的实现，不仅可以支撑不同业务的数据需求，而且也隐含着不同数据结构在性能、空间效率等方面的差异，从而导致不同的value操作之间存在着差异。</p><h3 id="做什么操作"><a href="#做什么操作" class="headerlink" title="做什么操作"></a>做什么操作</h3><p>对于一个数据库，基本操作无外乎增删改查。</p><ul><li>SET：新写入或更新一个key-value对；</li><li>GET：根据一个key读取相应的value值；</li><li>DELETE：根据一个key删除整个key-value对。</li></ul><p>在实际的业务场景中，我们经常会碰到这种情况：查询一个用户在一段时间内的访问记录。这种操作在键值数据库中属于SCAN操作，即<strong>根据一段key的范围返回相应的value值</strong>。因此，<strong>PUT&#x2F;GET&#x2F;DELETE&#x2F;SCAN是一个键值数据库的基本操作集合</strong>。</p><h2 id="键值对保存在哪？"><a href="#键值对保存在哪？" class="headerlink" title="键值对保存在哪？"></a>键值对保存在哪？</h2><p><strong>键值对保存在内存还是外存</strong>？</p><ul><li><p>内存优缺：读写很快。潜在的风险是一旦掉电，所有的数据都会丢失。</p></li><li><p>外存优缺：可避免数据丢失，但读写慢。</p></li></ul><p><strong>如何进行设计选择，我们通常需要考虑键值数据库的主要应用场景</strong>。比如，缓存场景下的数据需要能快速访问但允许丢失，那么，用于此场景的键值数据库通常采用内存保存键值数据。Memcached和Redis都是属于内存键值数据库。</p><p>为了和Redis保持一致，我们的SimpleKV就采用内存保存键值数据。</p><h2 id="如何定位键值对的位置"><a href="#如何定位键值对的位置" class="headerlink" title="如何定位键值对的位置"></a>如何定位键值对的位置</h2><p>当SimpleKV解析了客户端发来的请求，知道了要进行的键值对操作，此时，SimpleKV需要查找所要操作的键值对是否存在，这依赖于键值数据库的索引模块。<strong>索引的作用是让键值数据库根据key找到相应value的存储位置，进而执行操作</strong>。</p><p>内存键值数据库（例如Redis）采用哈希表作为索引，很大一部分原因在于，其键值数据基本都是保存在内存中的，而内存的高性能随机访问特性可以很好地与哈希表O(1)的操作复杂度相匹配。</p><p>对于Redis而言，它的value支持多种类型，当我们通过索引找到一个key所对应的value后，仍然需要从value的复杂结构（例如集合和列表）中进一步找到我们实际需要的数据，这个操作的效率本身就依赖于它们的实现结构。</p><p>对于PUT和DELETE两种操作来说，除了新写入和删除键值对，还需要分配和释放内存。这就涉及到SimpleKV的存储模块了。</p><h2 id="如何实现重启后快速提供服务"><a href="#如何实现重启后快速提供服务" class="headerlink" title="如何实现重启后快速提供服务"></a>如何实现重启后快速提供服务</h2><p>我们希望SimpleKV重启后能快速重新提供服务，所以，需要在SimpleKV的存储模块中增加持久化功能。</p><p>SimpleKV可以直接采用了文件形式，将键值数据通过调用本地文件系统的操作接口保存在磁盘上。此时，SimpleKV只需要考虑何时将内存中的键值数据保存到文件中，就可以了。</p><p>一种方式是，对于每一个键值对，SimpleKV都对其进行落盘保存，这虽然让SimpleKV的数据更加可靠，但是，因为每次都要写盘，SimpleKV的性能会受到很大影响。</p><p>另一种方式是，SimpleKV只是周期性地把内存中的键值数据保存到文件中，这样可以避免频繁写盘操作的性能影响。但是，一个潜在的代价是SimpleKV的数据仍然有丢失的风险。</p><p>和SimpleKV一样，Redis也提供了持久化功能。不过，为了适应不同的业务场景，Redis为持久化提供了诸多的执行机制和优化改进。</p><h2 id="基本内部架构"><a href="#基本内部架构" class="headerlink" title="基本内部架构"></a>基本内部架构</h2><p>一个键值数据库包括了<strong>访问框架、索引模块、操作模块和存储模块</strong>四部分。</p><img src="/2022/06/28/Getting-to-know-redis/simple_kv_structure.png" class=""><p>访问模式通常有两种：一种是<strong>通过函数库调用的方式供外部应用使用</strong>，比如，上图中的libsimplekv.so，就是以动态链接库的形式链接到我们自己的程序中，提供键值存储功能；另一种是<strong>通过网络框架以Socket通信的形式对外提供键值对操作</strong>，这种形式可以提供广泛的键值存储服务。在上图中，我们可以看到，网络框架中包括Socket Server和协议解析。</p><p>Memcached和Redis是通过网络框架访问。</p><p>采用网络访问模式会存在这样的问题：网络连接的处理、网络请求的解析，以及数据存取的处理，是用一个线程、多个线程，还是多个进程来交互处理呢？该如何进行设计和取舍呢？</p><p>如果一个线程既要处理网络连接、解析请求，又要完成数据存取，一旦某一步操作发生阻塞，整个线程就会阻塞住，这就降低了系统响应速度。如果我们采用不同线程处理不同操作，那么，某个线程被阻塞时，其他线程还能正常运行。但是，不同线程间如果需要访问共享资源，那又会产生线程竞争，也会影响系统效率。</p><h2 id="从SimpleKV到Redis"><a href="#从SimpleKV到Redis" class="headerlink" title="从SimpleKV到Redis"></a>从SimpleKV到Redis</h2><img src="/2022/06/28/Getting-to-know-redis/from_simple_kv_to_redis.png" class=""><p>从这张对比图中，我们可以看到，从SimpleKV演进到Redis，有以下几个重要变化：</p><ul><li>Redis主要通过网络框架进行访问，而不再是动态库了，这也使得Redis可以作为一个基础性的网络服务进行访问，扩大了Redis的应用范围。</li><li>Redis数据模型中的value类型很丰富，因此也带来了更多的操作接口。</li><li>Redis的持久化模块能支持两种方式：日志（AOF）和快照（RDB），这两种持久化方式具有不同的优劣势，影响到Redis的访问性能和可靠性。</li><li>SimpleKV是个简单的单机键值数据库，但是，Redis支持高可靠集群和高可扩展集群，因此，Redis中包含了相应的集群功能支撑模块。</li></ul><h1 id="数据结构：快速的Redis由哪些慢操作？"><a href="#数据结构：快速的Redis由哪些慢操作？" class="headerlink" title="数据结构：快速的Redis由哪些慢操作？"></a>数据结构：快速的Redis由哪些慢操作？</h1><h1 id="高性能IO模型：为什么单线程Redis能那么快？"><a href="#高性能IO模型：为什么单线程Redis能那么快？" class="headerlink" title="高性能IO模型：为什么单线程Redis能那么快？"></a>高性能IO模型：为什么单线程Redis能那么快？</h1><p>首先，我们需要明确一点：Redis是单线程，主要是指<strong>Redis的网络IO和键值对读写是由一个线程来完成的，这也是Redis对外提供键值存储服务的主要流程</strong>。但Redis的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。</p><h2 id="为啥用单线程？"><a href="#为啥用单线程？" class="headerlink" title="为啥用单线程？"></a>为啥用单线程？</h2><p><strong>多线程的开销</strong>：</p><p>我们刚开始增加线程数时，系统吞吐率会增加，但是，再进一步增加线程时，系统吞吐率就增长迟缓了，有时甚至还会出现下降的情况。</p><p>原因在于：解决多线程带来的共享资源的并发访问问题会产生额外的开销。</p><p>除此之外，采用多线程开发一般会引入同步原语来保护共享资源的并发访问，这也会降低系统代码的易调试性和可维护性。为了避免这些问题，Redis直接采用了单线程模式。</p><h2 id="Redis基于多路复用的高性能I-x2F-O模型"><a href="#Redis基于多路复用的高性能I-x2F-O模型" class="headerlink" title="Redis基于多路复用的高性能I&#x2F;O模型"></a>Redis基于多路复用的高性能I&#x2F;O模型</h2><blockquote><p>参考：<a href="https://juejin.cn/post/7049148028875178020">https://juejin.cn/post/7049148028875178020</a></p></blockquote><p>Linux中的IO多路复用机制是指一个线程处理多个IO流，<strong>该机制允许内核中，同时存在多个监听套接字和已连接套接字</strong>。内核会一直监听这些套接字上的连接请求或数据请求。一旦有请求到达，就会交给Redis线程处理，这就实现了一个Redis线程处理多个IO流的效果。</p><p>select&#x2F;epoll一旦监测到FD上有请求到达时，就会触发相应的事件。这些事件会被放进一个事件队列，Redis单线程对该事件队列不断进行处理。这样一来，Redis无需一直轮询是否有请求实际发生，这就可以避免造成CPU资源浪费。同时，Redis在对事件队列中的事件进行处理时，会调用相应的处理函数，这就实现了基于事件的回调。因为Redis一直在对事件队列进行处理，所以能及时响应客户端请求，提升Redis的响应性能。如图所示。</p><img src="/2022/06/28/Getting-to-know-redis/redis_io_multiplex.png" class=""><h1 id="AOF日志-amp-RDB快照：宕机了，Redis如何避免数据丢失，如何实现数据恢复？"><a href="#AOF日志-amp-RDB快照：宕机了，Redis如何避免数据丢失，如何实现数据恢复？" class="headerlink" title="AOF日志&amp;RDB快照：宕机了，Redis如何避免数据丢失，如何实现数据恢复？"></a>AOF日志&amp;RDB快照：宕机了，Redis如何避免数据丢失，如何实现数据恢复？</h1><p>由于Redis将数据存储在内存中，那面临了一个问题：<strong>一旦服务器宕机，内存中的数据将全部丢失。</strong></p><p>目前，Redis的持久化主要有两大机制，即AOF日志和RDB快照。</p><h2 id="AOF-Append-Only-File-日志"><a href="#AOF-Append-Only-File-日志" class="headerlink" title="AOF(Append Only File)日志"></a>AOF(Append Only File)日志</h2><h3 id="为何采用“写后日志”？"><a href="#为何采用“写后日志”？" class="headerlink" title="为何采用“写后日志”？"></a>为何采用“写后日志”？</h3><p>AOF日志：“写后日志“。Redis先执行命令，把数据写入内存，然后才记录日志。</p><p>问题：为何使用“写后日志“？</p><p>答：为了避免额外的检查开销，Redis在向AOF里面记录日志的时候，并不会先去对AOF日志命令进行语法检查。”写后日志“避免出现记录错误命令的情况。</p><h3 id="三种写回策略"><a href="#三种写回策略" class="headerlink" title="三种写回策略"></a>三种写回策略</h3><p>AOF的两个潜在风险：</p><ul><li>命令丢失导致无法恢复数据。如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。</li><li>阻塞后续操作。AOF日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。</li></ul><p>因此AOF有三种写回策略：</p><ul><li><strong>Always</strong>，同步写回：每个写命令执行完，立马同步地将日志写回磁盘；</li><li><strong>Everysec</strong>，每秒写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘；</li><li><strong>No</strong>，操作系统控制的写回：每个写命令执行完，只是先把日志写到AOF文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。</li></ul><table><thead><tr><th>配置项</th><th>写回时机</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td>Always</td><td>同步写回</td><td>可靠性高，数据基本不丢失</td><td>每个写命令都要落盘，性能影响较大</td></tr><tr><td>EverySec</td><td>每秒写回</td><td>性能适中</td><td>宕机时丢失1秒内的数据</td></tr><tr><td>No</td><td>操作系统控制的写回</td><td>性能好</td><td>宕机时丢失数据较多</td></tr></tbody></table><h3 id="AOF文件过大带来的问题与解决方案"><a href="#AOF文件过大带来的问题与解决方案" class="headerlink" title="AOF文件过大带来的问题与解决方案"></a>AOF文件过大带来的问题与解决方案</h3><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>随着接收的写命令越来越多，AOF文件会越来越大，会带来以下的问题：</p><ul><li>文件系统本身对文件大小有限制，无法保存过大的文件</li><li>如果文件太大，之后再往里面追加命令记录的话，效率也会变低</li><li>如果发生宕机，AOF中记录的命令要一个个被重新执行，用于故障恢复，如果日志文件太大，整个恢复过程就会非常缓慢，这就会影响到Redis的正常使用。</li></ul><h4 id="AOF重写机制"><a href="#AOF重写机制" class="headerlink" title="AOF重写机制"></a>AOF重写机制</h4><p>为解决这个问题，AOF有个重写机制：读取目前数据库中的所有键值对，然后对每一个键值对用一条命令记录它的写入。比如说，当读取了键值对“testkey”: “testvalue”之后，重写机制会记录set testkey testvalue这条命令。这样，当需要恢复时，可以重新执行该命令，实现“testkey”: “testvalue”的写入。</p><h4 id="AOF重写会阻塞主线程吗？"><a href="#AOF重写会阻塞主线程吗？" class="headerlink" title="AOF重写会阻塞主线程吗？"></a>AOF重写会阻塞主线程吗？</h4><p>和AOF日志由主线程写回不同，重写过程是由后台线程bgrewriteaof来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。</p><p>重写的步骤：</p><ul><li>每次执行重写时，主线程fork出后台的bgrewriteaof子进程。此时，fork会把主线程的内存拷贝一份给bgrewriteaof子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。</li><li>如果有写操作，Redis不仅会把这个操作写到正在使用的AOF日志的缓冲区，还会把这个日志写到AOF重写日志的缓冲区。</li></ul><p>如图所示：</p><img src="/2022/06/28/Getting-to-know-redis/aof_rewrite.png" class=""><p>总结：每次AOF重写时，Redis会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为Redis采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。</p><h2 id="RDB-Redis-DataBase-快照"><a href="#RDB-Redis-DataBase-快照" class="headerlink" title="RDB(Redis DataBase)快照"></a>RDB(Redis DataBase)快照</h2><p>用AOF方法进行故障恢复的时候，需要逐一把操作日志都执行一遍。如果操作日志非常多，Redis就会恢复得很缓慢，影响到正常使用。因此有另一种持久化方法：RDB快照。</p><p>RDB（Redis DataBase）：内存快照，记录内存中的数据在某一个时刻的状态。</p><p>需要考虑的问题有：</p><ul><li>对哪些数据做快照？这关系到快照的执行效率问题；</li><li>做快照时，数据还能被增删改吗？这关系到Redis是否被阻塞，能否同时正常处理请求。</li></ul><h3 id="要给哪些数据做快照？"><a href="#要给哪些数据做快照？" class="headerlink" title="要给哪些数据做快照？"></a>要给哪些数据做快照？</h3><p>Redis的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是<strong>全量快照</strong>，也就是说，把内存中的所有数据都记录到磁盘中。</p><p>由于全量数据量会很大，因此我们需要考虑的一个问题是：<strong>快照是否会阻塞主线程。</strong></p><p>Redis提供了两个命令来生成RDB文件，分别是save和bgsave。</p><ul><li>save：在主线程中执行，会导致阻塞；</li><li>bgsave：创建一个子进程，专门用于写入RDB文件，避免了主线程的阻塞，这也是Redis RDB文件生成的默认配置。</li></ul><p>我们可以使用bgsave来执行全量快照，这既提供了数据的可靠性保证，也避免了对Redis的性能影响。</p><h3 id="快照时能修改数据吗？"><a href="#快照时能修改数据吗？" class="headerlink" title="快照时能修改数据吗？"></a>快照时能修改数据吗？</h3><p>由于全量数据可能比较大，做完快照所需要的时间可能比较长，在这个时间间隔内，如果内存数据被进行了修改，快照的正确性就得不到保证；如果内存数据不允许被修改，也就是Redis这段期间无法提供写服务，也会给业务服务造成影响。</p><p>为了在执行快照的时候让redis也能执行写操作，<strong>Redis借助了操作系统提供的写时复制技术（Copy-On-Write, COW），在执行快照的同时，正常处理写操作。</strong></p><p>bgsave子进程是由主线程fork生成的，可以共享主线程的所有内存数据。bgsave子进程运行后，开始读取主线程的内存数据，并把它们写入RDB文件。如果主线程对这些数据也都是读操作（例如图中的键值对A），那么，主线程和bgsave子进程相互不影响。但是，如果主线程要修改一块数据（例如图中的键值对C），那么，这块数据就会被复制一份，生成该数据的副本。然后，bgsave子进程会把这个副本数据写入RDB文件，而在这个过程中，主线程仍然可以直接修改原来的数据。</p><p>如图所示：</p><img src="/2022/06/28/Getting-to-know-redis/rdb_write.png" class=""><h3 id="多久做一次快照？"><a href="#多久做一次快照？" class="headerlink" title="多久做一次快照？"></a>多久做一次快照？</h3><p>假设全量快照执行的时间间隔过长，在间隔内，如果机器宕机了，那恢复后的数据可能不是最新的（即，在间隔内修改的数据因为没有快照记录而无法恢复）。</p><p>如果全量快照执行的时间间过短，也会带来两方面的开销：</p><ul><li>频繁将全量数据写入磁盘，会给磁盘带来很大压力，多个快照竞争有限的磁盘带宽，前一个快照还没有做完，后一个又开始做了，容易造成恶性循环。</li><li>bgsave子进程需要通过fork操作从主线程创建出来。虽然，子进程在创建后不会再阻塞主线程，但是，fork这个创建过程本身会阻塞主线程，而且主线程的内存越大，阻塞时间越长。如果频繁fork出bgsave子进程，这就会频繁阻塞主线程了。</li></ul><p>解决方案：可以做增量快照。在第一次做完全量快照后，后续时刻如果再做快照，我们只需要将被修改的数据写入快照文件就行。</p><p>如何记录被修改的数据？可以使用使用额外的元数据信息去记录哪些数据被修改了，但这会带来额外的空间开销问题。<strong>“记录哪些数据被修改了”这个思路和AOF就很类似。</strong></p><p>Redis 4.0中提出了一个<strong>混合使用AOF日志和内存快照</strong>的方法。简单来说，内存快照以一定的频率执行，在两次快照之间，使用AOF日志记录这期间的所有命令操作。</p><p>这样一来，快照不用很频繁地执行，这就避免了频繁fork对主线程的影响。而且，AOF日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。</p><p>如图所示：</p><img src="/2022/06/28/Getting-to-know-redis/aof_cooperate_with_rdb.png" class=""><h1 id="数据同步：主从库如何实现数据一致？"><a href="#数据同步：主从库如何实现数据一致？" class="headerlink" title="数据同步：主从库如何实现数据一致？"></a>数据同步：主从库如何实现数据一致？</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>尽管AOF和RDB可以在Redis宕机后恢复数据，但假设我们在使用过程中只运行了一个Redis实例，那么该实例在宕机期间，是无法服务新来的数据存取请求的。</p><p>Redis具有高可靠性表现在：</p><ul><li><strong>数据尽量少丢失</strong>。AOF和RDB保证了这一点。</li><li><strong>服务尽量少中断</strong>。实现方案是，<strong>增加副本冗余量</strong>，将一份数据同时保存在多个实例上。即使有一个实例出现了故障，需要过一段时间才能恢复，其他实例也可以对外提供服务，不会影响业务使用。</li></ul><h2 id="主从库模式介绍"><a href="#主从库模式介绍" class="headerlink" title="主从库模式介绍"></a>主从库模式介绍</h2><p>Redis提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式：</p><ul><li><strong>读操作</strong>：主库、从库都可以接收；</li><li><strong>写操作</strong>：首先到主库执行，然后，主库将写操作同步给从库。</li></ul><h2 id="主从库数据同步的原理"><a href="#主从库数据同步的原理" class="headerlink" title="主从库数据同步的原理"></a>主从库数据同步的原理</h2><h3 id="同步的步骤"><a href="#同步的步骤" class="headerlink" title="同步的步骤"></a>同步的步骤</h3><p>当我们启动多个Redis实例的时候，它们相互之间就可以通过replicaof（Redis 5.0之前使用slaveof）命令形成主库和从库的关系，之后会按照三个阶段完成数据的第一次同步：</p><ul><li><strong>从库和主库建立连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了</strong>。</li><li><strong>主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载</strong>。</li><li>主库会把第二阶段执行过程中新收到的写命令，再发送给从库。</li></ul><p>举例而言，如图所示：</p><img src="/2022/06/28/Getting-to-know-redis/master_slave_sync.png" class=""><p>第一步：从库发送psync命令，表示要进行数据同步，主库根据这个命令的参数来启动复制。psync命令包含了<strong>主库的runID</strong>（redis实例的唯一标识符）和<strong>复制进度offset</strong>（-1表示第一次复制）两个参数。</p><p>主库收到psync命令后，会用FULLRESYNC（<strong>表示第一次复制采用的全量复制</strong>）响应命令带上两个参数：主库runID和主库目前的复制进度offset，返回给从库。从库收到响应后，会记录下这两个参数。</p><p>第二步：主库执行bgsave命令，生成RDB文件，接着将文件发给从库。从库接收到RDB文件后，会先清空当前数据库，然后加载RDB文件。</p><p>在主库将数据同步给从库的过程中，主库不会被阻塞，仍然可以正常接收请求。但是，这些请求中的写操作并没有记录到刚刚生成的RDB文件中。为了保证主从库的数据一致性，主库会在内存中用专门的replication buffer，记录RDB文件生成后收到的所有写操作。</p><p>第三步：当主库完成RDB文件发送后，就会把此时replication buffer中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。</p><h3 id="主从级联"><a href="#主从级联" class="headerlink" title="主从级联"></a>主从级联</h3><p>一次全量复制中，对于主库来说，需要完成两个耗时的操作：生成RDB文件和传输RDB文件。如果从库数量很多，而且都要和主库进行全量复制的话，就会产生以下两个问题：</p><ul><li>多次fork，会阻塞主线程处理正常请求，从而导致主库响应应用程序的请求速度变慢。</li><li>传输RDB文件也会占用主库的网络带宽，同样会给主库的资源使用带来压力。</li></ul><p>解决方案是：<strong>通过“主-从-从”模式将主库生成RDB和传输RDB的压力，以级联的方式分散到从库上</strong>。</p><p>如图所示：</p><img src="/2022/06/28/Getting-to-know-redis/cascade.png" class=""><p><strong>一旦主从库完成了全量复制，它们之间就会一直维护一个网络连接，主库会通过这个连接将后续陆续收到的命令操作再同步给从库</strong>，这个过程也称为<strong>基于长连接的命令传播</strong>，可以避免频繁建立连接的开销。</p><p>注意：第一次同步，全量复制是无法避免的，因此<strong>一个Redis实例的数据库不要太大</strong>，一个实例大小在几GB级别比较合适，这样可以减少RDB文件生成、传输和重新加载的开销。</p><h3 id="主从库断网了该怎么办？"><a href="#主从库断网了该怎么办？" class="headerlink" title="主从库断网了该怎么办？"></a>主从库断网了该怎么办？</h3><p>当主从库断连后，主库会把断连期间收到的写操作命令，写入replication buffer，同时也会把这些操作命令也写入repl_backlog_buffer这个缓冲区。repl_backlog_buffer是一个<strong>环形缓冲区</strong>，<strong>主库会记录自己写到的位置，从库则会记录自己已经读到的位置</strong>。需要注意的一点是，如果从库的读取速度比较慢，就有可能导致从库还未读取的操作被主库新写的操作覆盖了，这会导致主从库间的数据不一致。（可以通过调整<strong>repl_backlog_size</strong>这个参数来降低这种风险）</p><p>主从库的连接恢复之后，从库首先会给主库发送psync命令，并把自己当前的slave_repl_offset发给主库，主库会判断自己的master_repl_offset和slave_repl_offset之间的差距。在网络断连阶段，主库可能会收到新的写操作命令，所以，一般来说，master_repl_offset会大于slave_repl_offset。此时，主库只用把master_repl_offset和slave_repl_offset之间的命令操作同步给从库就行。</p><p>如图所示：</p><img src="/2022/06/28/Getting-to-know-redis/network_off_solution.png" class=""><h1 id="哨兵机制-amp-哨兵集群：主库挂了，如何不间断服务？哨兵挂了，主从库怎么切？"><a href="#哨兵机制-amp-哨兵集群：主库挂了，如何不间断服务？哨兵挂了，主从库怎么切？" class="headerlink" title="哨兵机制&amp;哨兵集群：主库挂了，如何不间断服务？哨兵挂了，主从库怎么切？"></a>哨兵机制&amp;哨兵集群：主库挂了，如何不间断服务？哨兵挂了，主从库怎么切？</h1><h2 id="哨兵机制"><a href="#哨兵机制" class="headerlink" title="哨兵机制"></a>哨兵机制</h2><p>如果主库挂了，我们就需要运行一个新主库，比如说把一个从库切换为主库，把它当成主库。这就涉及到三个问题：</p><ul><li>如何判断主库是真挂了？</li><li>该选择哪个从库作为主库？</li><li>怎么把新主库的相关信息通知给从库和客户端呢？</li></ul><h3 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h3><p>哨兵其实就是一个运行在特殊模式下的Redis进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。</p><ul><li>监控：哨兵进程在运行时，周期性地给所有的主从库发送PING命令，检测它们是否仍然在线运行。如果某库没有在规定时间内响应哨兵的PING命令，哨兵就会把它标记为“主观下线”状态。</li><li>选主：主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。</li><li>通知：在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行replicaof命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。</li></ul><h3 id="判定下线"><a href="#判定下线" class="headerlink" title="判定下线"></a>判定下线</h3><p>哨兵可能会对某库的在线状态产生误判。如果哨兵对主库的下线状态产生误判了，启动了主从切换，后续的选主和通知操作都会带来额外的计算和通信开销。</p><p>误判一般会发生在集群网络压力较大、网络拥塞，或者是主库本身压力较大的情况下。</p><p>为了减小误判率，<strong>通常会采用多实例组成的集群模式进行部署，这也被称为哨兵集群</strong>，由哨兵集群一起决策某库是否下线。当大多数的哨兵实例都判断主库已经“主观下线”了，主库才会被标记为“客观下线”。（少数服从多数）</p><h3 id="选定新主库"><a href="#选定新主库" class="headerlink" title="选定新主库"></a>选定新主库</h3><p>哨兵选择新主库的过程可以被称为“筛选+打分”：</p><ul><li>先按照<strong>一定的筛选条件</strong>，把不符合条件的从库去掉。</li><li>然后，我们再按照<strong>一定的规则</strong>，给剩下的从库逐个打分，将得分最高的从库选为新主库。</li></ul><p>筛选的条件：该从库在线，切网络连接状态稳定。</p><p>打分规则可以分为三轮，<strong>只要在某一轮中，有从库得分最高，那么它就是主库了，选主过程到此结束。</strong>如果没有出现得分最高的从库，那么就继续进行下一轮。这三个轮规则分别是：</p><ul><li><strong>优先级最高的从库得分高。</strong>用户可以通过slave-priority配置项，给不同的从库设置不同优先级。用户可以手动给内存大的实例设置一个高优先级。</li><li><strong>和旧主库同步程度最接近的从库得分高。</strong>如果在所有从库中，有从库的slave_repl_offset最接近master_repl_offset，那么它的得分就最高，可以作为新主库。</li><li><strong>ID号（实例唯一标识符）小的从库得分高。</strong></li></ul><h2 id="哨兵集群"><a href="#哨兵集群" class="headerlink" title="哨兵集群"></a>哨兵集群</h2><p>如果有哨兵实例在运行时发生了故障，主从库还能正常切换吗？</p><p>一旦多个实例组成了<strong>哨兵集群</strong>，即使有哨兵实例出现故障挂掉了，其他哨兵还能继续协作完成主从库切换的工作。</p><h3 id="基于pub-x2F-sub机制的哨兵集群组成"><a href="#基于pub-x2F-sub机制的哨兵集群组成" class="headerlink" title="基于pub&#x2F;sub机制的哨兵集群组成"></a>基于pub&#x2F;sub机制的哨兵集群组成</h3><p>在配置哨兵的信息时，我们只需要设置主库的IP和端口，并没有配置其他哨兵的连接信息。<strong>哨兵实例如何知道彼此的地址？</strong></p><p>哨兵只要和主库建立起了连接，就可以在主库上发布消息了，比如说发布它自己的连接信息（IP和端口）。同时，它也可以从主库上订阅消息，获得其他哨兵发布的连接信息。当多个哨兵实例都在主库上做了发布和订阅操作后，它们之间就能知道彼此的IP地址和端口。</p><p><strong>哨兵是如何知道从库的IP地址和端口的呢？</strong></p><p>哨兵向主库发送INFO命令来完成的。哨兵给主库发送INFO命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。</p><h3 id="基于pub-x2F-sub机制的客户端事件通知"><a href="#基于pub-x2F-sub机制的客户端事件通知" class="headerlink" title="基于pub&#x2F;sub机制的客户端事件通知"></a>基于pub&#x2F;sub机制的客户端事件通知</h3><p>在“哨兵机制”这一节中，还有个问题没有解决：<strong>客户端如何得知主从切换的过程呢？</strong></p><p>从本质上说，哨兵就是一个运行在特定模式下的Redis实例，只不过它并不服务请求操作，只是完成监控、选主和通知的任务。所以，每个哨兵实例也提供pub&#x2F;sub机制，客户端可以从哨兵订阅消息。哨兵提供的消息订阅频道有很多，不同频道包含了主从库切换过程中的不同关键事件。</p><p>客户端读取哨兵的配置文件后，可以获得哨兵的地址和端口，和哨兵建立网络连接。然后，我们可以在客户端执行订阅命令，来获取不同的事件消息。</p><h3 id="由哪个哨兵执行主从切换？"><a href="#由哪个哨兵执行主从切换？" class="headerlink" title="由哪个哨兵执行主从切换？"></a>由哪个哨兵执行主从切换？</h3><p>首先，先介绍一下“客观下线”的判断过程：</p><ul><li>任何一个实例只要自身判断主库“主观下线”后，就会给其他实例发送is-master-down-by-addr命令。接着，其他实例会根据自己和主库的连接情况，做出Y或N的响应，Y相当于赞成票，N相当于反对票。</li><li>一个哨兵获得了仲裁所需的赞成票数后，就可以标记主库为“客观下线”。这个所需的赞成票数是通过哨兵配置文件中的quorum配置项设定的。</li><li>此时，这个哨兵就可以再给其他哨兵发送命令，表明希望由自己来执行主从切换，并让所有其他哨兵进行投票。这个投票过程称为“Leader选举”。因为最终执行主从切换的哨兵称为Leader，投票过程就是确定Leader。</li></ul><p>在投票过程中，任何一个想成为Leader的哨兵，要满足两个条件：第一，拿到半数以上的赞成票；第二，拿到的票数同时还需要大于等于哨兵配置文件中的quorum值。</p><p>如果一轮投票没有产生Leader，哨兵集群会等待一段时间（也就是哨兵故障转移超时时间的2倍），再重新选举。哨兵集群能够进行成功投票，很大程度上依赖于选举命令的正常网络传播。如果网络压力较大或有短时堵塞，就可能导致没有一个哨兵能拿到半数以上的赞成票。所以，等到网络拥塞好转之后，再进行投票选举，成功的概率就会增加。</p><p>注意：最好保证所有哨兵实例的配置是一致的，尤其是主观下线的判断值down-after-milliseconds。否则，可能哨兵集群一直没有对有故障的主库形成共识，也就没有及时切换主库，最终的结果就是集群服务不稳定。</p><h1 id="切片集群：数据增多了，是该加内存还是加实例？"><a href="#切片集群：数据增多了，是该加内存还是加实例？" class="headerlink" title="切片集群：数据增多了，是该加内存还是加实例？"></a>切片集群：数据增多了，是该加内存还是加实例？</h1><h2 id="如何保存更多数据？"><a href="#如何保存更多数据？" class="headerlink" title="如何保存更多数据？"></a>如何保存更多数据？</h2><p>有两种方案：纵向扩展（scale up）和横向扩展（scale out）：</p><ul><li><strong>纵向扩展</strong>：升级单个Redis实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的CPU。<ul><li>优点：<strong>实施起来简单、直接</strong>。</li><li>缺点：纵向扩展会受到硬件和成本的限制。除此之外，当使用RDB对数据进行持久化时，如果数据量增加，需要的内存也会增加，主线程fork子进程时就可能会阻塞。</li></ul></li><li><strong>横向扩展</strong>：横向增加当前Redis实例的个数。<ul><li>优点：不用担心单个实例的硬件和成本限制。<strong>在面向百万、千万级别的用户规模时，横向扩展的Redis切片集群会是一个非常好的选择</strong>。</li></ul></li></ul><p>但切片集群面临着这样的问题：</p><ul><li>数据切片后，在多个实例之间如何分布？</li><li>客户端怎么确定想要访问的数据在哪个实例上？</li></ul><h2 id="数据切片和实例的对应分布关系"><a href="#数据切片和实例的对应分布关系" class="headerlink" title="数据切片和实例的对应分布关系"></a>数据切片和实例的对应分布关系</h2><p>Redis Cluster方案采用哈希槽（Hash Slot），来处理数据和实例之间的映射关系。在Redis Cluster方案中，一个切片集群共有16384个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的key，被映射到一个哈希槽中。我们在部署Redis Cluster方案时，可以使用cluster create命令创建集群，此时，Redis会自动把这些槽平均分布在集群实例上。例如，如果集群中有N个实例，那么，每个实例上的槽个数为16384&#x2F;N个。我们也可以使用cluster meet命令手动建立实例间的连接，形成集群，再使用cluster addslots命令，指定每个实例上的哈希槽个数。</p><h2 id="客户端如何定位数据？"><a href="#客户端如何定位数据？" class="headerlink" title="客户端如何定位数据？"></a>客户端如何定位数据？</h2><p>一般来说，客户端和集群实例建立连接后，实例就会把哈希槽的分配信息发给客户端。但是，在集群刚刚创建的时候，每个实例只知道自己被分配了哪些哈希槽，是不知道其他实例拥有的哈希槽信息的。Redis实例会把自己的哈希槽信息发给和它相连接的其它实例，来完成哈希槽分配信息的扩散。当实例之间相互连接后，每个实例就有所有哈希槽的映射关系了。客户端收到哈希槽信息后，会把哈希槽信息缓存在本地。当客户端请求键值对时，会先计算键所对应的哈希槽，然后就可以给相应的实例发送请求了。</p><p>但是，在集群中，实例和哈希槽的对应关系并不是一成不变的，最常见的变化有两个：</p><ul><li>在集群中，实例有新增或删除，Redis需要重新分配哈希槽；</li><li>为了负载均衡，Redis需要把哈希槽在所有实例上重新分布一遍。</li></ul><p>Redis Cluster方案提供了一种<strong>重定向机制，</strong>所谓的“重定向”，就是指，客户端给一个实例发送数据读写操作时，这个实例上并没有相应的数据，那么，这个实例就会给客户端返回MOVED命令响应结果，这个结果中就包含了新实例的访问地址，客户端接着再给一个新实例发送操作请求。</p><p>假设数据还没有迁移完毕，客户端就发送了请求，则不会返回MOVED响应结果。举例说明，Slot 2正在从实例2往实例3迁移，key1和key2已经迁移过去，key3和key4还在实例2。客户端向实例2请求key2后，就会收到实例2返回的ASK命令。ASK命令表示两层含义：第一，表明Slot数据还在迁移中；第二，ASK命令把客户端所请求数据的最新实例地址返回给客户端，此时，客户端需要给实例3发送ASKING命令，然后再发送操作命令。</p><p>和MOVED命令不同，<strong>ASK命令并不会更新客户端缓存的哈希槽分配信息</strong>。</p><h1 id="其它问题"><a href="#其它问题" class="headerlink" title="其它问题"></a>其它问题</h1><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h1&gt;&lt;blockquote&gt;
&lt;p&gt; 本文基本是参考了《Redis核心技术与实战》的基础篇。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;本文需要搞明白的问题有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本架构：一个键值数据库包含什么？&lt;/li&gt;
&lt;li&gt;数据结构：快速的Redis有哪些慢操作？&lt;/li&gt;
&lt;li&gt;高性能IO模型：为什么单线程Redis能那么快？&lt;/li&gt;
&lt;li&gt;AOF日志&amp;amp;RDB快照：宕机了，Redis如何避免数据丢失，如何实现数据恢复？&lt;/li&gt;
&lt;li&gt;数据同步：主从库如何实现数据一致？&lt;/li&gt;
&lt;li&gt;哨兵机制&amp;amp;哨兵集群：主库挂了，如何不间断服务？哨兵挂了，主从库怎么切？&lt;/li&gt;
&lt;li&gt;切片集群：数据增多了，是该加内存还是加实例？&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Redis" scheme="http://example.com/categories/Redis/"/>
    
    
  </entry>
  
  <entry>
    <title>关于HTTP不可不说的那些事</title>
    <link href="http://example.com/2022/05/13/HTTP/"/>
    <id>http://example.com/2022/05/13/HTTP/</id>
    <published>2022-05-13T11:23:49.000Z</published>
    <updated>2022-06-25T09:21:45.173Z</updated>
    
    <content type="html"><![CDATA[<p>本文主要聚焦以下内容：</p><ol><li>HTTP简要介绍</li><li>HTTPS简要介绍</li><li>HTTP简要发展史</li></ol><span id="more"></span><h1 id="HTTP简介"><a href="#HTTP简介" class="headerlink" title="HTTP简介"></a>HTTP简介</h1><p>HTTP(HyperText Transfer Protocol)，超文本传输协议。</p><h2 id="常见状态码"><a href="#常见状态码" class="headerlink" title="常见状态码"></a>常见状态码</h2><blockquote><p>参考：<a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status">https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Status</a></p></blockquote><ul><li>1xx：提示信息 （实践中还没遇到过…）</li><li>2xx：成功响应<ul><li>200：OK，一切正常，响应头有Body数据</li><li>204：No Content，对于该请求没有的内容可发送，但头部字段可能有用，即响应头无body数据</li><li>206：Partial Content，返回一部分数据，用于HTTP分块下载或者续传。表示请求已成功，并且主体包含所请求的数据区间，该数据区间是在请求的 <a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Range"><code>Range</code></a> 首部指定的。</li></ul></li><li>3xx：资源变动，重定向。<ul><li>301： Moved Permanently，说明请求的资源已经被移动到了由 <a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Location"><code>Location</code></a> 头部指定的 url 上，是固定的不会再改变。需要使用新的url来访问。（浏览器会自动跳转）</li><li>302：  Found，表明请求的资源被暂时的移动到了由该 HTTP 响应的响应头 <a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Location"><code>Location</code></a> 指定的 URL 上。需要使用新的url来访问。（浏览器会自动跳转）</li><li>304：Not Modified，说明无需再次传输请求的内容，也就是说可以使用缓存的内容。</li></ul></li><li>4xx：客户端发送的报文错误<ul><li>400：Bad Request，表示由于语法无效，服务器无法理解该请求。</li><li>403：Forbidden，指的是服务器端有能力处理该请求，但是拒绝授权访问。</li><li>404：Not Found，服务器端无法找到所请求的资源。</li></ul></li><li>5xx：服务器错误<ul><li>500：Internal Server Error，表示服务器端错误的响应状态码，意味着所请求的服务器遇到意外的情况并阻止其执行请求。通用的“万能”响应代码。</li><li>501：Not Implemented，表示请求的方法不被服务器支持，因此无法被处理。服务器必须支持的方法（即不会返回这个状态码的方法）只有 <a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods/GET"><code>GET</code></a> 和 <a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods/HEAD"><code>HEAD</code></a>。</li><li>502：Bad Gateway，表示作为网关或代理角色的服务器，从上游服务器中接收到的响应是无效的。</li><li>503：Service Unavailable，表示服务器尚未处于可以接受请求的状态，无法响应请求。</li></ul></li></ul><h2 id="常见HEADERS字段"><a href="#常见HEADERS字段" class="headerlink" title="常见HEADERS字段"></a>常见HEADERS字段</h2><blockquote><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers">https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers</a></p></blockquote><ul><li>Host请求头：指明了请求将要发送到的服务器主机名和端口号。如果没有包含端口号，会自动使用被请求服务的默认端口（比如HTTPS URL 使用 443 端口，HTTP URL 使用 80 端口）</li><li>Content-Length：指明发送给接收方的消息主体的大小，单位是字节，用十进制表示。</li><li>Connection：决定当前的事务完成后，是否会关闭网络连接。如果该值是“keep-alive”，网络连接就是持久的，不会关闭，使得对同一个服务器的请求可以继续在该连接上完成，即复用该TCP连接。注：HTTP&#x2F;1.1版本默认连接都是持久的，但为了兼容老版本，还是需要指明“keep-alive”。</li><li>Content-Type：告诉客户端实际返回的内容的内容类型。客户端请求时，通过Accept字段声明自己可以接受哪些数据格式。</li><li>Content-Encoding：指明服务器返回的数据使用了什么压缩格式。户端请求时，通过Accept-Encoding字段声明自己可以接受哪些压缩方法。</li></ul><h2 id="GET和POST"><a href="#GET和POST" class="headerlink" title="GET和POST"></a>GET和POST</h2><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><blockquote><p><a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods">https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Methods</a></p></blockquote><ul><li>GET：请求指定的资源。使用 <code>GET</code> 的请求应该只用于获取数据。请求无body。</li><li>POST：发送数据给服务器。请求主体的类型由 <a href="https://developer.mozilla.org/zh-CN/docs/Web/HTTP/Headers/Content-Type"><code>Content-Type</code></a> 首部指定，内容放在body里。</li></ul><h3 id="GET和POST是安全和幂等的吗？"><a href="#GET和POST是安全和幂等的吗？" class="headerlink" title="GET和POST是安全和幂等的吗？"></a>GET和POST是安全和幂等的吗？</h3><blockquote><p>幂等的解释：<a href="https://developer.mozilla.org/zh-CN/docs/Glossary/Idempotent">https://developer.mozilla.org/zh-CN/docs/Glossary/Idempotent</a></p></blockquote><ul><li>安全：请求方法不会破坏服务器上的资源。我的理解是，请求方法不会对服务器上的资源进行增删改操作（写操作）。</li><li>幂等：同样的请求被执行一次与连续执行多次的效果是一样的，服务器的状态也是一样的。</li></ul><p>根据这两个定义，我们可以看到：</p><ol><li>GET方法<strong>是安全而且幂等</strong>的，因为GET方法实际上只是对服务器上的资源进行了读操作，并且每次执行的结果都是相等的。</li><li>POST方法<strong>不是安全</strong>的，因为POST方法会修改服务器上的资源；POST方法也<strong>不是幂等</strong>的，因为多次提交数据会产生多个资源，比如购物网站会提示请勿重复提交订单。</li></ol><h2 id="Cookie、Session和Token"><a href="#Cookie、Session和Token" class="headerlink" title="Cookie、Session和Token"></a>Cookie、Session和Token</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><blockquote><p><a href="https://juejin.cn/post/6844904034181070861">https://juejin.cn/post/6844904034181070861</a></p></blockquote><p>HTTP是无状态的，也就是说这一次请求和上一次请求是没有关联的。由此会产生一个问题：在进行有关联的一系列请求操作时，应该怎么办？例如，我在一个网站购物，我可能会进行以下操作：登录 -&gt; 加入购物车 -&gt; 下单结算 -&gt; 支付，这里的每一个操作都需要用户的身份信息。Cookie和Session就是为了解决请求相关联（同一域名下的网页能够共享某些数据）的问题。</p><p>为了解决网页共享数据的问题，可以有这样的解决方案：我们将该域名下的用户信息存储在客户端里，用户每次请求该域名的网页数据时带上这个用户信息即可。但这样有一个问题：客户端存储大小有限，并且由于是存储在客户端，用户可以随意修改信息，不够安全。因此，cookie - session机制诞生了。</p><p>session包含了用户信息，存储在服务端，而cookie包含了session id，存储在客户端。客户端访问服务器的流程如下：</p><img src="/2022/05/13/HTTP/cookie.png" class=""><h4 id="cookie和session的区别"><a href="#cookie和session的区别" class="headerlink" title="cookie和session的区别"></a>cookie和session的区别</h4><blockquote><p>有效期参考：<a href="https://blog.csdn.net/u010002184/article/details/89413166">https://blog.csdn.net/u010002184/article/details/89413166</a></p></blockquote><ul><li>安全性：cookie客户端；session服务端。session胜。</li><li>存储值类型不同：cookie字符串；session都可。</li><li>有效期不同。会话cookie（无到期时间）存在内存里；持久cookie（有到期时间）存在磁盘中。session由服务端设置过期时间。</li><li>存储大小不同。</li></ul><h3 id="cookie面临的攻击"><a href="#cookie面临的攻击" class="headerlink" title="cookie面临的攻击"></a>cookie面临的攻击</h3><blockquote><p><a href="https://tech.meituan.com/2018/10/11/fe-security-csrf.html">https://tech.meituan.com/2018/10/11/fe-security-csrf.html</a></p></blockquote><h4 id="CSRF"><a href="#CSRF" class="headerlink" title="CSRF"></a>CSRF</h4><p>CSRF（Cross-site request forgery）跨站请求伪造：攻击者诱导受害者进入第三方网站，在第三方网站中，向被攻击网站发送跨站请求。利用受害者在被攻击网站已经获取的注册凭证，绕过后台的用户验证，达到冒充用户对被攻击的网站执行某项操作的目的。</p><p>可以看到，CSRF的两个特点：</p><ul><li>CSRF（通常）发生在第三方域名。</li><li>CSRF攻击者不能获取到Cookie等信息，只是使用。</li></ul><p>因此防护策略如下：</p><ul><li>阻止不明外域的访问：同源检测、Samesite Cookie</li><li>提交时要求附加本域才能获取的信息：CSRF token、双重Cookie验证</li></ul><h3 id="Token"><a href="#Token" class="headerlink" title="Token"></a>Token</h3><h4 id="Acesss-Token"><a href="#Acesss-Token" class="headerlink" title="Acesss Token"></a>Acesss Token</h4><p>访问资源接口（API）时所需要的资源凭证，流程如下：</p><img src="/2022/05/13/HTTP/token.png" class=""><p>每一次请求都需要携带 token，需要把 token 放到 HTTP 的 Header 里。基于 token 的用户认证是一种服务端无状态的认证方式，服务端不用存放 token 数据。用解析 token 的计算时间换取 session 的存储空间，从而减轻服务器的压力，减少频繁的查询数据库，token 完全由应用管理，所以它可以避开同源策略。</p><h4 id="Refresh-Token"><a href="#Refresh-Token" class="headerlink" title="Refresh Token"></a>Refresh Token</h4><p>refresh token 是专用于刷新 access token 的 token，流程如下：</p><h3 id="Token和Session区别"><a href="#Token和Session区别" class="headerlink" title="Token和Session区别"></a>Token和Session区别</h3><blockquote><p>OAuth参考资料：</p></blockquote><ul><li>状态：session服务端记录会话状态；token服务端无会话状态</li><li>安全性：token更高。</li><li>如果你的用户数据可能需要和第三方共享，或者允许第三方调用 API 接口，用 Token （OAuth）。如果永远只是自己的网站，用什么就无所谓了。</li></ul><h3 id="HTTP缓存"><a href="#HTTP缓存" class="headerlink" title="HTTP缓存"></a>HTTP缓存</h3><blockquote><p><a href="https://juejin.cn/post/6844903838768431118">https://juejin.cn/post/6844903838768431118</a></p></blockquote><p><strong>强制缓存</strong></p><p>利用Cache-Control字段实现。流程如下：</p><ul><li>当浏览器第一次请求访问服务器资源时，服务器会在返回这个资源的同时，在 Response 头部加上 Cache-Control，Cache-Control 中设置了过期时间大小；</li><li>浏览器再次请求访问服务器中的该资源时，会先<strong>通过请求资源的时间与 Cache-Control 中设置的过期时间大小，来计算出该资源是否过期</strong>，如果没有，则使用该缓存，否则重新请求服务器；</li><li>服务器再次收到请求后，会再次更新 Response 头部的 Cache-Control。</li></ul><p><strong>协商缓存</strong></p><p>向服务器发送请求，服务器会根据这个请求的request header的一些参数来判断是否命中协商缓存，如果命中，则返回304状态码并带上新的response header通知浏览器从缓存中读取资源。</p><p>可以利用字段<code>ETag</code> 和 <code>If-None-Match</code>  ：</p><ul><li>响应头部中 <code>Etag</code>：唯一标识响应资源；</li><li>请求头部中的 <code>If-None-Match</code>：当资源过期时，浏览器发现响应头里有 Etag，则再次向服务器发起请求时，会将请求头If-None-Match 值设置为 Etag 的值。服务器收到请求后进行比对，如果资源没有变化返回 304，如果资源变化了返回 200。</li></ul><p>也可利用字段<code>Last-Modified</code>和<code>If-Modified-Since</code>：</p><ul><li>响应头部中的 <code>Last-Modified</code>：标示这个响应资源的最后修改时间；</li><li>请求头部中的 <code>If-Modified-Since</code>：当资源过期了（浏览器判断Cache-Control标识的max-age过期），发现响应头中具有 Last-Modified 声明，则再次发起请求的时候带上if-modified-since，表示请求时间，服务器收到请求后发现有 If-Modified-Since 则与被请求资源的最后修改时间进行对比（Last-Modified），如果最后修改时间较新（大），说明资源又被改过，则返回最新资源，HTTP 200 OK；如果最后修改时间较旧（小），说明资源无新修改，响应 <strong>HTTP 304</strong> 走缓存（状态码304的场景！）。</li></ul><p>浏览器第一次请求时流程：</p><img src="/2022/05/13/HTTP/first_request.png" class=""><p>第二次请求相同网页时：</p><img src="/2022/05/13/HTTP/second_request.png" class=""><h1 id="HTTPS简介"><a href="#HTTPS简介" class="headerlink" title="HTTPS简介"></a>HTTPS简介</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>HTTP可能存在以下问题：</p><ul><li>被窃听。http明文传输。</li><li>被篡改。中间人攻击。</li><li>认证问题。服务器身份可能为假；客户端身份为假；</li></ul><p>超文本传输安全协议（英语：Hypertext Transfer Protocol Secure，缩写：HTTPS，常称为HTTP over TLS，HTTP over SSL或HTTP Secure）是一种通过计算机网络进行安全通信的传输协议。HTTPS经由HTTP进行通信，但利用SSL&#x2F;TLS来加密数据包。HTTPS开发的主要目的，是提供对网站服务器的身份认证，保护交换数据的隐私与完整性。</p><p>HTTPS特点：</p><ul><li>机密性：传输密文</li><li>完整性：Hash算法</li><li>身份认证：数字证书</li></ul><h2 id="HTTPS-连接建立过程"><a href="#HTTPS-连接建立过程" class="headerlink" title="HTTPS 连接建立过程"></a>HTTPS 连接建立过程</h2><blockquote><p><a href="https://juejin.cn/post/6844903608421449742">https://juejin.cn/post/6844903608421449742</a></p></blockquote><p>首先需要先建立TCP连接。</p><p>其次进行SSL握手，流程如下：</p><img src="/2022/05/13/HTTP/ssl_handshake.png" class=""><h1 id="HTTP简要发展史（性能相关）"><a href="#HTTP简要发展史（性能相关）" class="headerlink" title="HTTP简要发展史（性能相关）"></a>HTTP简要发展史（性能相关）</h1><h2 id="HTTP-x2F-1-1与HTTP-x2F-1-0相比"><a href="#HTTP-x2F-1-1与HTTP-x2F-1-0相比" class="headerlink" title="HTTP&#x2F;1.1与HTTP&#x2F;1.0相比"></a>HTTP&#x2F;1.1与HTTP&#x2F;1.0相比</h2><ul><li>长连接：keep-alive</li><li>支持pipeline（用户端并行发请求，但服务端处理请求还是串行，服务端需要按顺序响应收到的请求，如果服务端处理某个请求消耗的时间比较长，那么只能等相应完这个请求后， 才能处理下一个请求，这是HTTP 层队头阻塞的问题。实际上pipeline功能基本没人用）。</li></ul><h2 id="HTTP-x2F-2与HTTP-x2F-1-1相比"><a href="#HTTP-x2F-2与HTTP-x2F-1-1相比" class="headerlink" title="HTTP&#x2F;2与HTTP&#x2F;1.1相比"></a>HTTP&#x2F;2与HTTP&#x2F;1.1相比</h2><blockquote><p>参考：<a href="https://juejin.cn/post/6844903734670000142">https://juejin.cn/post/6844903734670000142</a></p></blockquote><h3 id="二进制分帧"><a href="#二进制分帧" class="headerlink" title="二进制分帧"></a>二进制分帧</h3><p>头信息和数据体都是二进制编码，并且统称为帧（frame）：头信息帧（Headers Frame）和数据帧（Data Frame），如图所示：</p><img src="/2022/05/13/HTTP/http2_data.png" class=""><h3 id="多路复用"><a href="#多路复用" class="headerlink" title="多路复用"></a>多路复用</h3><blockquote><p>http2队头阻塞参考：<a href="https://zhuanlan.zhihu.com/p/32553477">https://zhuanlan.zhihu.com/p/32553477</a></p></blockquote><p>三个概念:</p><ul><li>流（Stream）：已建立的TCP连接上的双向字节流，可以承载一个或多个消息。</li><li>消息（Message）：一个完整的HTTP请求或响应，由一个或多个帧组成。特定消息的帧在同一个流上发送，这意味着一个HTTP请求或响应只能在一个流上发送。</li><li>帧（Frame）：通信的基本单位。<br> 一个TCP连接上可以有任意数量的流。</li></ul><p>HTTP2建立一个TCP连接，一个连接上面可以有任意多个流（stream），消息分割成一个或多个帧在流里面传输。帧传输过去以后，再进行重组，形成一个完整的请求或响应。如图所示：</p><img src="/2022/05/13/HTTP/http2_multiplex.png" class=""><p>HTTP&#x2F;2.0传输层仍然是TCP，我理解仍然会有队头阻塞（TCP层）的现象存在。TCP 是字节流协议，TCP 层必须保证收到的字节数据是完整且连续的，这样内核才会将缓冲区里的数据返回给 HTTP 应用，那么当「前 1 个字节数据」没有到达时，后收到的字节数据只能存放在内核缓冲区里，只有等到这 1 个字节数据到达时，HTTP&#x2F;2 应用层才能从内核中拿到数据。如图所示：</p><img src="/2022/05/13/HTTP/http2_blocking.png" class=""><h3 id="头部压缩"><a href="#头部压缩" class="headerlink" title="头部压缩"></a>头部压缩</h3><p>如果你同时发出多个请求，他们的头是一样的或是相似的，那么，协议会帮你消除重复的部分。</p><p>这就是所谓的 <code>HPACK</code> 算法：在客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了。</p><h3 id="服务器推送"><a href="#服务器推送" class="headerlink" title="服务器推送"></a>服务器推送</h3><p>服务器端推送使得服务器可以预测客户端需要的资源，主动推送到客户端。例如：客户端请求index.html，服务器端能够额外推送script.js和style.css。 实现原理就是客户端发出页面请求时，服务器端能够分析这个页面所依赖的其他资源，主动推送到客户端。</p><h2 id="QUIC与HTTP-x2F-2相比"><a href="#QUIC与HTTP-x2F-2相比" class="headerlink" title="QUIC与HTTP&#x2F;2相比"></a>QUIC与HTTP&#x2F;2相比</h2><blockquote><p>参考：<a href="https://zhuanlan.zhihu.com/p/32553477">https://zhuanlan.zhihu.com/p/32553477</a></p></blockquote><p>QUIC（Quick UDP Internet Connections），直译过来就是“快速的 UDP 互联网连接”，是 Google 基于 UDP 提出的一种改进的通信协议，作为传统 HTTP over TCP 的替代品。</p><h3 id="没有队头阻塞的多路复用"><a href="#没有队头阻塞的多路复用" class="headerlink" title="没有队头阻塞的多路复用"></a>没有队头阻塞的多路复用</h3><p>QUIC 的多路复用和 HTTP2 类似。在一条 QUIC 连接上可以并发发送多个 HTTP 请求 (stream)。但是 QUIC 的多路复用相比 HTTP2 有一个很大的优势。QUIC 一个连接上的多个 stream 之间没有依赖。这样假如 stream2 丢了一个 udp packet，也只会影响 stream2 的处理。不会影响 stream2 之前及之后的 stream 的处理。如图所示：</p><p>并不是所有的 QUIC 数据都不会受到队头阻塞的影响，比如 QUIC 当前也是使用 Hpack 压缩算法，由于算法的限制，丢失一个头部数据时，可能遇到队头阻塞。总体来说，QUIC 在传输大量数据时，比如视频，受到队头阻塞的影响很小。</p><h3 id="连接建立延时低"><a href="#连接建立延时低" class="headerlink" title="连接建立延时低"></a>连接建立延时低</h3><p>传输层 0RTT 就能建立连接；加密层 0RTT 就能建立加密连接。</p><p>对于 HTTP&#x2F;1 和 HTTP&#x2F;2 协议，TCP 和 TLS 是分层的，分别属于内核实现的传输层、openssl 库实现的表示层，因此它们难以合并在一起，需要分批次来握手，先 TCP 握手，再 TLS 握手。TCP握手约1.5RTT，TLS握手约1.5RTT，总共握手大约耗时3RTT。</p><p>而QUIC 协议内部包含了 TLS，它在自己的帧会携带 TLS数据，再加上 QUIC 使用的是 TLS&#x2F;1.3，因此仅需 1 个 RTT 就可以同时完成建立连接与密钥协商。甚至在第二次连接的时候，应用数据包可以和 QUIC 握手信息（连接信息 + TLS 信息）一起发送。</p><h3 id="连接迁移"><a href="#连接迁移" class="headerlink" title="连接迁移"></a>连接迁移</h3><p>基于 TCP 传输协议的 HTTP 协议，由四元组（源 IP、源端口、目的 IP、目的端口）确定一条 TCP 连接，那么当移动设备的网络从 4G 切换到 WIFI 时，意味着 IP 地址变化了，那么就必须要断开连接，然后重新建立连接。</p><p>QUIC 协议不再以四元组来标识一条连接，而是以一个 连接ID 来进行标识，这样就算 IP 或者端口发生变化时，只要 ID 不变，这条连接依然维持着，上层业务逻辑感知不到变化，不会中断，也就不需要重连。从而实现了“连接迁移”。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要聚焦以下内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HTTP简要介绍&lt;/li&gt;
&lt;li&gt;HTTPS简要介绍&lt;/li&gt;
&lt;li&gt;HTTP简要发展史&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>关于TCP协议的一些疑惑</title>
    <link href="http://example.com/2022/05/04/TCPConfusion/"/>
    <id>http://example.com/2022/05/04/TCPConfusion/</id>
    <published>2022-05-04T12:26:53.000Z</published>
    <updated>2022-05-15T15:54:53.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文需要搞清楚的问题有：</p><ol><li>为何TCP协议握手要设计成三次？为何TCP协议挥手需要四次？TCP握手过程中出现问题了会怎样？</li><li>为什么需要TIME_WAIT状态？为何TIME_WAIT等待时间是2MSL？TIME_WAIT过多的危害？如何优化TIME_WAIT？</li><li>SYN攻击是什么？如何避免？</li><li>IP层的MTU是什么？TCP为什么需要MSS？</li><li>如果已经建立了连接，客户端突然故障了怎么办？</li><li>重传机制中的SACK（Selective Acknowledgment 选择性确认）是什么？Duplicate SACK是什么？</li><li>TCP 是如何解决窗口关闭时，潜在的死锁现象呢？</li><li>流量控制中的糊涂窗口综合征是什么？如何避免？</li></ol><span id="more"></span><h1 id="TCP基本概况"><a href="#TCP基本概况" class="headerlink" title="TCP基本概况"></a>TCP基本概况</h1><p>TCP状态变迁图如图所示（摘自《TCP&#x2F;IP详解》）</p><p>在建立连接的三次握手过程中，服务端和客户端的状态变迁分别为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">客户端closed--发syn--&gt;SYN_SENT--收syn, ack,发ack--&gt;客户端established</span><br><span class="line">服务端closed--&gt;listen--收到syn, 发syn, ack--&gt;SYN收到--收ack--&gt;服务端established</span><br></pre></td></tr></table></figure><p>在断开连接的四次分手过程中，服务端和客户端的状态迁移分别为：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">服务端established--收fin, 发ack--&gt;close_wait --应用进程关闭, 发fin--&gt; last_ack --收ack--&gt;服务端closed</span><br><span class="line">客户端established--应用进程关闭,发fin--&gt;fin_wait_1--收ack--&gt;fin_wait_2--收fin,发ack--&gt;time_wait,2MSL--&gt;closed</span><br></pre></td></tr></table></figure><h1 id="答疑解惑"><a href="#答疑解惑" class="headerlink" title="答疑解惑"></a>答疑解惑</h1><blockquote><p>参考：<a href="https://www.cnblogs.com/traditional/p/12936575.html">https://www.cnblogs.com/traditional/p/12936575.html</a></p></blockquote><h2 id="为何TCP协议握手要设计成三次？"><a href="#为何TCP协议握手要设计成三次？" class="headerlink" title="为何TCP协议握手要设计成三次？"></a>为何TCP协议握手要设计成三次？</h2><p>首先，两个实体建立连接，至少需要两个步骤。举例来说，实体A要和实体B建立良好的通信关系，则第一步A要主动告诉B它想通信了，第二步则是B回复A它可以建立通信。但这个例子是建立在实体A和实体B的接受能力和发送能力良好且两者通信顺畅的情况下。</p><p>如果TCP协议握手只设计成两次的话，会产生以下的问题：</p><ol><li><strong>防止旧的初始化报文造成混乱</strong>。举例来说，假设现在客户端给服务端发送了一个SYN报文，记作旧报文，但由于网络或者种种其它问题，在规定时间内客户端没有收到来自服务端的报文，因此客户端又发送了一个SYN报文，记作新报文。此时服务端先收到了旧报文，给客户端回了一个SYN+ACK报文，称为旧回复报文，此后服务端又收到了新报文，给客户端又回了个新回复报文。假设客户端先收到新回复报文，与B建立了连接；此后收到旧回复报文，并将之丢弃。服务端将无法感知客户端丢弃了旧回复报文，可能一直处于等待数据的状态，浪费资源。</li><li>双方初始化序列号未达成共识。还是上面的例子，假设服务端先发的新报文的回复a，再发的旧报文的回复b，此时服务端期待拿到b中序列号的ack；但客户端会丢弃回复b，后续传数据时会用a中携带的服务端的序列号，此时就出现了双方没有就服务端的初始序列号达成共识，会造成通信混乱。</li></ol><p>当TCP协议握手三次的时候，如果客户端收到旧回复报文，则会给服务端发送RST包告诉服务端异常终止连接。</p><h2 id="为何TCP协议挥手需要四次？"><a href="#为何TCP协议挥手需要四次？" class="headerlink" title="为何TCP协议挥手需要四次？"></a>为何TCP协议挥手需要四次？</h2><ul><li>客户端发送fin只是为了告诉服务端自己已经没有数据要发送了，客户端仍然可以接收数据。此时服务端可能还有数据要处理或者还有数据要发送给客户端，因此服务端返回ack来表明自己已经知道客户端没数据发了。</li><li>等到服务端处理完后，发送fin告诉客户端自己也没有需要发送的数据了，可以正式关闭连接，客户端需要回复ack给服务端来表明自己已经知道服务端没有需要发送的数据了。</li></ul><p>可以看到，服务端的ack和fin是分开发的，因此挥手需要四次。</p><h2 id="为什么需要TIME-WAIT状态？为何TIME-WAIT等待时间是2MSL？TIME-WAIT过多的危害？"><a href="#为什么需要TIME-WAIT状态？为何TIME-WAIT等待时间是2MSL？TIME-WAIT过多的危害？" class="headerlink" title="为什么需要TIME_WAIT状态？为何TIME_WAIT等待时间是2MSL？TIME_WAIT过多的危害？"></a>为什么需要TIME_WAIT状态？为何TIME_WAIT等待时间是2MSL？TIME_WAIT过多的危害？</h2><blockquote><p> 参考《TCP&#x2F;IP详解》18.6.1 2MSL等待状态</p></blockquote><h3 id="需要TIME-WAIT的原因"><a href="#需要TIME-WAIT的原因" class="headerlink" title="需要TIME_WAIT的原因"></a>需要TIME_WAIT的原因</h3><p>主动关闭的一方需要TIME_WAIT状态的原因：</p><ul><li>保证最后向被动关闭的一方发送的ack能被对方接收到，从而能够正确的关闭连接。</li><li>防止旧报文影响新连接。假设没有TIME_WAIT状态，双方都进入到了closed状态。此后socket被复用，双方建立了新连接，可能会接收到这个旧报文，导致数据错乱。</li></ul><p>假设客户端为主动关闭的一方。如果没有TIME_WAIT，此时客户端进入到了closed状态，假设客户端发送的最后一个ack丢失，则服务端会一直处于last_ack的状态，没有正常关闭。</p><p>正是因为有了TIME_WAIT状态，假设客户端最后一个ack丢失，服务端在发送fin后一段时间没有收到ack，会重新发送fin数据包给客户端，客户端在TIME_WAIT阶段收到FIN包后，就会重新发ack给服务端。因此服务端可以通过这种机制正常关闭。</p><h3 id="TIME-WAIT为2MSL的原因"><a href="#TIME-WAIT为2MSL的原因" class="headerlink" title="TIME_WAIT为2MSL的原因"></a>TIME_WAIT为2MSL的原因</h3><p>MSL，报文段最大生存时间（Maximum Segment Lifetime），是任何报文段在被丢弃之前在网络内存活的最长时间。</p><p>TIME_WAIT的等待时间设置为2MSL是为了保证服务端能够收到最后的ack，客户端发送ack到ack消亡需要MSL时间，而收到服务端重传的fin又需要MSL时间，因此就是2MSL时间。</p><p>具体而言就是，从服务端第一次发送fin包的时候就开始设置超时重传定时器。当客户端发送的ack丢失时服务端的定时器必然已经到期，服务端第二次发送fin包。客户端从发ack到ack丢包经历的时间&lt;&#x3D;MSL，从服务端再次发fin包到客户端收到fin包经历的时间&lt;&#x3D;MSL，因此保证客户端等待2MSL的时间内会收到服务端重发的fin包。当客户端收到fin包后，会再次重启2MSL计时器。</p><p>问题：如果服务端重发的fin包也丢包了怎么办？</p><p>答：连续两次丢包的概率太小了，因此忽略这种情况比解决这种情况更有性价比。</p><h3 id="TIME-WAIT过多的危害"><a href="#TIME-WAIT过多的危害" class="headerlink" title="TIME_WAIT过多的危害"></a>TIME_WAIT过多的危害</h3><p>如果服务端有TCP连接处于TIME_WAIT状态，则说明是服务端主动断开的。倘若服务端TIME_WAIT状态过多，则有以下问题：</p><ul><li>对端口资源占用。当处于TIME_WAIT状态时，这个端口就不能被其它新连接使用了，而服务端端口资源时有限的。</li><li>占用内存资源。</li></ul><h2 id="SYN攻击是什么？如何避免？"><a href="#SYN攻击是什么？如何避免？" class="headerlink" title="SYN攻击是什么？如何避免？"></a>SYN攻击是什么？如何避免？</h2><p>假设攻击者短时间伪造不同 IP 地址的 SYN 报文，服务端每接收到一个 SYN 报文，就进入SYN_RCVD 状态，但服务端发送出去的 ACK + SYN 报文，无法得到未知 IP 主机的 ACK 应答，久而久之就会占满服务端的 SYN 接收队列（半连接队列），使得服务器不能为正常用户服务。</p><h3 id="正常流程"><a href="#正常流程" class="headerlink" title="正常流程"></a>正常流程</h3><ol><li><p>当服务端接收到客户端的 SYN 报文时，会将其加入到内核的「SYN 队列」；</p></li><li><p>接着发送 SYN + ACK 给客户端，等待客户端回应 ACK 报文；</p></li><li><p>服务端接收到 ACK 报文后，将连接从「SYN 队列」移除并放入到「Accept 队列」；</p></li><li><p>应用通过调用 socket 接口 accpet()，从「Accept 队列」取出连接；</p></li></ol><h3 id="SYN攻击时"><a href="#SYN攻击时" class="headerlink" title="SYN攻击时"></a>SYN攻击时</h3><p>第2、3步被阻塞，SYN队列被填满，队列中没有元素被取出，Accept 队列被应用消耗完后一直为空，应用程序取不到新连接。</p><h3 id="避免方式"><a href="#避免方式" class="headerlink" title="避免方式"></a>避免方式</h3><p>打开<strong>tcp_syncookies</strong>，达到以下效果：</p><ol><li><p>当 「SYN 队列」满之后，后续服务器收到 SYN 包，不进入「SYN 队列」，而是计算出一个 cookie 值，再以 SYN + ACK 中的「序列号」的形式返回客户端；</p></li><li><p>服务端接收到客户端的应答报文时，服务器会检查这个 ACK 包的合法性。如果合法，直接放入到「Accept 队列」；</p></li><li><p>最后应用通过调用 socket 接口 accpet()，从「 Accept 队列」取出连接</p></li></ol><h2 id="如果已经建立了连接，客户端突然故障了怎么办？"><a href="#如果已经建立了连接，客户端突然故障了怎么办？" class="headerlink" title="如果已经建立了连接，客户端突然故障了怎么办？"></a>如果已经建立了连接，客户端突然故障了怎么办？</h2><blockquote><p>参考《TCP&#x2F;IP详解》第23章 保活定时器</p></blockquote><p>保活功能主要是为服务器应用程序提供的。如果一个客户端突然断电了，那就会在服务端留下一个半开放的连接，服务端一直会等待来自客户端的数据。保活功能能让服务端检测这种半开放的连接。</p><p>如果某个特定的连接在2小时内没有任何动作，那么服务端会向客户端发送一个探测报文段。可能会出现以下几种情况：</p><ol><li>客户主机正常运行，TCP响应正常。服务端知道客户主机正常后，会将保活定时器复位。</li><li>客户主机已经崩溃，处于关闭状态或者正在重启；或者客户主机正常运行，但不可达。此时客户TCP没有响应。服务端没收到响应，就会每隔75s发送一个探测，共发10个探测。如果服务端发送的探测一个响应都没都收到，服务端就会认为客户主机已经关闭了，就会终止连接。</li><li>客户主机已经重新启动了。这时候客户主机会回复一个RST的报文，让服务端终止连接。</li></ol><h2 id="IP层的MTU是什么？TCP为什么需要MSS？"><a href="#IP层的MTU是什么？TCP为什么需要MSS？" class="headerlink" title="IP层的MTU是什么？TCP为什么需要MSS？"></a>IP层的MTU是什么？TCP为什么需要MSS？</h2><p>MTU：最大传输单元（Maximum transmission unit）。一般网络接口都会定义一个MTU，如果IP包的尺寸&lt;&#x3D;MTU，则这个数据包会原封不动从这个网络接口发送，否则需要分段。</p><p>MSS：最大报文长度（Maximum Segment Size）。</p><p>当TCP发送一个SYN时，将MSS值设置为外出接口的MTU长度减去固定的IP首部和TCP首部长度。MSS让主机限制另一端发送数据报的长度，如图所示，最终sun和slip不会发送超过256字节数据的报文段。较小MTU连接到另一个网络上的主机因此避免了分段。（参考《TCP&#x2F;IP详解》18.4 最大报文段长度）。</p><p>不过这个方法不能完全避免分段，因为存在以下的情况：假设两端主机都连接到以太网上，都采用536字节的MSS，但中间网络采用的MTU为296字节，这样也会出现分段。为了解决这个问题，需要使用路径MTU发现机制。（参考《TCP&#x2F;IP详解》24.2 路径MTU发现）</p><p>路径MTU发现：IP首部有个DF（Don’t Fragment）标志位，表示不要分片。如果IP包的长度超过了某个路由器的MTU，但设置了DF标志位，则路由器丢弃该数据包，并产生一个“不能分片”的ICMP错误发送给源主机，告诉源主机其MTU时多少。</p><h2 id="重传机制中的SACK（Selective-Acknowledgment-选择性确认）是什么？Duplicate-SACK是什么？"><a href="#重传机制中的SACK（Selective-Acknowledgment-选择性确认）是什么？Duplicate-SACK是什么？" class="headerlink" title="重传机制中的SACK（Selective Acknowledgment 选择性确认）是什么？Duplicate SACK是什么？"></a>重传机制中的SACK（Selective Acknowledgment 选择性确认）是什么？Duplicate SACK是什么？</h2><blockquote><p><a href="https://blog.csdn.net/wdscq1234/article/details/52503315">https://blog.csdn.net/wdscq1234/article/details/52503315</a></p></blockquote><h3 id="SACK"><a href="#SACK" class="headerlink" title="SACK"></a>SACK</h3><p>当我们在收到一个失序的报文段时，TCP会立刻产生一个重复的ACK。但对方不知道一个重复的ack时由于报文丢失引起的，还是由于乱序引起的。对方采取的策略是，当它连续收到3个及以上重复ack的时候，就认为报文段丢失了，此时无需等待超时定时器到点，就立刻重传丢失的报文段。</p><p>问题在于，此时重传这一个报文，还是重传这个报文以及以后的所有报文？例如，客户端发了seg1<del>seg10，服务端返回了三个ack seg2，此时客户端是只重传seg2，还是seg2</del>seg10都要重传呢？</p><ul><li>倘若只重传seg2，那假如seg2~seg7都丢失了，客户端需要一个个等待超时才重发，浪费了时间。</li><li>倘若seg2~seg10都重传了，那假如只有seg2丢失了，多重传了这几个包，浪费了流量。</li></ul><p>SACK是TCP的一个选项（RFC 2018），它允许TCP单独确认非连续的片段，用来告知真正丢失的包。双方设备需要同时支持SACK才可以使用这个功能。SACK选项中包含一系列没有确认的数据的序列范围。</p><h3 id="D-SACK"><a href="#D-SACK" class="headerlink" title="D-SACK"></a>D-SACK</h3><blockquote><p><a href="https://blog.csdn.net/u014023993/article/details/85041321">https://blog.csdn.net/u014023993/article/details/85041321</a></p></blockquote><p>RFC2883对SACK进行了扩展，称为D-SACK：使得扩展后的SACK具有通知发送端哪些数据被重复接收了。</p><p>D-SACK使用了SACK的第一个段来做标志，<strong>如何判断D-SACK</strong>：</p><ul><li>如果SACK的第一个段的范围被ACK所覆盖，那么就是D-SACK。如图所示，说明3000-3499被重复接收了，4000以前的数据也收到了。</li></ul><p><img src="https://img-blog.csdnimg.cn/20190127102442394.jpg" alt="img"></p><ul><li><p>如果SACK的第一个段的范围被SACK的第二个段覆盖，那么就是D-SACK。如图所示，[4000，SACK&#x3D;3000-3500, 4500-5000]说明4000前的数据已收到，3000-3500的数据重复收到，4000-4499的包丢失，4500-5000的包收到。</p><p><img src="https://img-blog.csdnimg.cn/2019012710245146.jpg" alt="img"></p></li></ul><p>上图中的[4000，SACK&#x3D;4500-5000]是普通的SACK，表明4000以前的数据已经收到，4500-5000的数据包已经收到但还未确认，即未收到4000-4499。</p><h2 id="TCP-是如何解决窗口关闭时，潜在的死锁现象呢？"><a href="#TCP-是如何解决窗口关闭时，潜在的死锁现象呢？" class="headerlink" title="TCP 是如何解决窗口关闭时，潜在的死锁现象呢？"></a>TCP 是如何解决窗口关闭时，潜在的死锁现象呢？</h2><blockquote><p>参考《TCP&#x2F;IP详解》第22章 TCP的坚持定时器</p></blockquote><p>接收方通过通告窗口来进行流量控制。如果窗口大小为 0 时，就会阻止发送方给接收方传递数据，直到窗口变为非 0 为止，这就是窗口关闭。当发送方窗口关闭后，接收方处理完缓冲区的数据，会发送一个ACK告诉发送方新窗口值，万一这个ACK丢掉了，那就会出现死锁，即：发送方等待ack来告知它可以继续发数据，而接收方等待接收数据。</p><p>为了防止这种死锁的情况发生，发送方使用坚持定时器（persist timer）来周期性向接收方查询窗口是否增大，这个用来查询的报文段叫做窗口探查（window probe）。这种方法可以用来解决潜在的死锁现象。</p><h2 id="流量控制中的糊涂窗口综合症是什么？如何避免？"><a href="#流量控制中的糊涂窗口综合症是什么？如何避免？" class="headerlink" title="流量控制中的糊涂窗口综合症是什么？如何避免？"></a>流量控制中的糊涂窗口综合症是什么？如何避免？</h2><blockquote><p>参考《TCP&#x2F;IP详解》22.3 糊涂窗口综合症</p><p><a href="https://blog.csdn.net/wdscq1234/article/details/52463952">https://blog.csdn.net/wdscq1234/article/details/52463952</a></p><p><strong>碎碎念：《TCP&#x2F;IP详解》真的不是机翻的吗…翻译实在是太差劲了</strong></p></blockquote><p>糊涂窗口综合症SWS（Silly Window Syndrome）（我觉得叫愚蠢窗口综合症更合适…）：通告窗口很小的时候，双方也会进行数据交换。举例来说，假设传输的数据很小，甚至比TCP+IP首部还小，那其实这样的传输就很浪费资源。这个现象可能发生在接收端，也可能发生在发送端。</p><p>可以采取以下措施：</p><ol><li>接收方不通告小窗口。当「窗口大小」小于 min(MSS, 缓存空间 &#x2F; 2)时，就会向发送方通告窗口为 0，也就阻止了发送方再发数据过来。等到接收方处理了一些数据后，窗口大小&gt;&#x3D;min(MSS, 缓存空间 &#x2F; 2)，就可以把通告窗口让发送方发送数据过来。</li><li>发送方避免发送小数据。<ul><li>待发送的数据为一个全尺寸的段；</li><li>TCP可以发送大小至少为另一端在此连接上通告的最大窗口大小的一半；</li><li>纳格算法启动的情况下，TCP 在没有急需 ACK 的数据的情况下，TCP 可以发送任何它必须立即发送的数据。</li></ul></li></ol><h2 id="多路IO复用？"><a href="#多路IO复用？" class="headerlink" title="多路IO复用？"></a>多路IO复用？</h2><h2 id="如果TCP握手过程出现问题了会怎么样？第一个消息丢包了会怎样？第二个消息丢包了会怎样？第三个消息丢包了会怎样？"><a href="#如果TCP握手过程出现问题了会怎么样？第一个消息丢包了会怎样？第二个消息丢包了会怎样？第三个消息丢包了会怎样？" class="headerlink" title="如果TCP握手过程出现问题了会怎么样？第一个消息丢包了会怎样？第二个消息丢包了会怎样？第三个消息丢包了会怎样？"></a>如果TCP握手过程出现问题了会怎么样？第一个消息丢包了会怎样？第二个消息丢包了会怎样？第三个消息丢包了会怎样？</h2>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;本文需要搞清楚的问题有：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;为何TCP协议握手要设计成三次？为何TCP协议挥手需要四次？TCP握手过程中出现问题了会怎样？&lt;/li&gt;
&lt;li&gt;为什么需要TIME_WAIT状态？为何TIME_WAIT等待时间是2MSL？TIME_WAIT过多的危害？如何优化TIME_WAIT？&lt;/li&gt;
&lt;li&gt;SYN攻击是什么？如何避免？&lt;/li&gt;
&lt;li&gt;IP层的MTU是什么？TCP为什么需要MSS？&lt;/li&gt;
&lt;li&gt;如果已经建立了连接，客户端突然故障了怎么办？&lt;/li&gt;
&lt;li&gt;重传机制中的SACK（Selective Acknowledgment 选择性确认）是什么？Duplicate SACK是什么？&lt;/li&gt;
&lt;li&gt;TCP 是如何解决窗口关闭时，潜在的死锁现象呢？&lt;/li&gt;
&lt;li&gt;流量控制中的糊涂窗口综合征是什么？如何避免？&lt;/li&gt;
&lt;/ol&gt;</summary>
    
    
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>TCP协议-从传统拥塞控制算法讲起</title>
    <link href="http://example.com/2022/04/23/TCP/"/>
    <id>http://example.com/2022/04/23/TCP/</id>
    <published>2022-04-23T13:31:21.000Z</published>
    <updated>2022-04-24T05:14:59.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>发生拥塞控制的原因：资源(带宽、交换节点的缓存、处理机)的需求&gt;可用资源。拥塞控制就是为了防止过多的数据注入到网络中，这样可以使网络中的路由器或者链路不至于过载。</p><p>本篇学习笔记需要理清的问题有以下几点：</p><ul><li><p>TCP RENO算法（教科书算法）是如何控制网络拥塞的？有什么局限性？</p></li><li><p>为了解决这个局限性，TCP的拥塞算法是如何改进的？</p></li><li><p>Google提出的BBR算法和TCP广泛使用的拥塞算法相比，有何不同？有何优点？</p></li></ul><span id="more"></span><h1 id="带宽时延乘积（bandwidth-delay-product）"><a href="#带宽时延乘积（bandwidth-delay-product）" class="headerlink" title="带宽时延乘积（bandwidth-delay product）"></a>带宽时延乘积（bandwidth-delay product）</h1><p>首先先介绍一下带宽时延乘积的概念。</p><blockquote><p>参考：<a href="https://blog.csdn.net/hackerwin7/article/details/21969307">https://blog.csdn.net/hackerwin7/article/details/21969307</a></p></blockquote><ul><li><p>带宽：指单位时间内从发送端到接收端所能通过的“最高数据率”，是一种硬件限制。TCP发送端和接收端的数据传输数率不可能超过两点间的带宽限制。类比于一条马路上的车道，车道数越多，每秒能并行前进的车辆也就越多；带宽也是如此，带宽越大，每秒能发送的数据量也就越大。</p></li><li><p>RTT（Round Trip Time）：表示从发送端到接收端的一去一回需要的时间，TCP在数据传输过程中会对RTT进行采样（即对发送数据到其收到ACK的时间差进行测量，并根据测量值更新RTT值），TCP根据得到的RTT值更新RTO（Retransmission TimeOut）值，即重传间隔。发送端对每个发出的数据包进行计时，如果在RTO时间内没有收到所发出的数据包的对应ACK，则认为数据包丢失，将重传数据。一般RTO值都比采样得到的RTT值要大。</p></li><li><p>带宽时延乘积：即带宽<em>RTT，该值等于发送端到接收端单向通道的数据容积的两倍，即放满单向网络通道的数据量为带宽</em>RTT&#x2F;2。</p></li></ul><p>设滑动窗口大小为W， 发送端和接收端的带宽为B， RTT为Tr。TCP发送数据时受滑动窗口的限制，当TCP将滑动窗口中的数据都发出后，在收到第一个ACK之前，可发送的滑动窗口大小是0，不能再发送数据了，必须等待ACK包使滑动窗口移动。在理想情况下，数据发出后的RTT时间后ACK包到达发送端，现在假设TCP在一个RTT时间内能发出的最大数据量为W，在不考虑带宽限制的情况下，TCP发包的最大速率是 V &#x3D; W&#x2F;Tr。</p><p>当V&lt;B时，带宽不构成瓶颈，此时速率的限制主要来源于窗口大小限制；当V&gt;B时，速率限制来源于带宽限制。</p><p>V与B的关系可以替换成W、Tr与B的关系，将 V &#x3D; W&#x2F;Tr带入可以得到，当W &lt;&#x3D; B<em>Tr，带宽不构成瓶颈；当W &gt; B</em>Tr时，带宽构成瓶颈。</p><p>B<em>Tr就是带宽时延乘积。取W为TCP能支持窗口的最大值Wmax，当Wmax &lt;&#x3D; B</em>Tr时，此时发送和接收端之间的通道就是所谓的<strong>长肥网络</strong>（LFN，long fat network），即带宽时延乘积较大的通道。在我们平时生活中使用的宽带网络，因为带宽都比较小，从而B*Tr也比较小，再加上网络情况比较复杂，拥塞情况比较常见，所以这些网络环境下，TCP速率的主要限制因素在于带宽，丢包率等。</p><p>在W&lt;B*Tr时，影响TCP发送数据速率的最直接的因素是滑动窗口的大小，TCP的拥塞控制策略是通过控制窗口大小来控制速率，而慢启动，拥塞避免这些算法实际上就是控制窗口增长方式的算法，也就是控制窗口增加的加速度大小。</p><h1 id="基于丢包的拥塞控制算法（Loss-based）"><a href="#基于丢包的拥塞控制算法（Loss-based）" class="headerlink" title="基于丢包的拥塞控制算法（Loss-based）"></a>基于丢包的拥塞控制算法（Loss-based）</h1><h2 id="RENO算法（教科书上的算法）"><a href="#RENO算法（教科书上的算法）" class="headerlink" title="RENO算法（教科书上的算法）"></a>RENO算法（教科书上的算法）</h2><p>TCP协议传统的拥塞控制算法为：慢开始、拥塞避免、快重传、快启动。</p><ul><li><p><strong>慢启动</strong>：主机开始发送数据的时候，如果立即将大量的数据注入到网络中，可能会出现网络的拥塞。慢启动算法就是在主机刚开始发送数据报的时候先探测一下网络的状况，如果网络状况良好，发送方每发送一次报文段都能正确接收到报文段ACK，就增加拥塞窗口（congestion window，简称cwnd）的大小，由1-&gt;2-&gt;4-&gt;8（以报文段长度为单位）这样指数级增加cwnd大小。</p></li><li><p><strong>拥塞避免</strong>：由于cwnd是指数级增长，为了防止cwnd增加过快而导致网络拥塞，需要设置一个慢启动的拥塞窗口阈值ssthresh。当cwnd &lt; ssthresh，使用慢启动算法；当cwnd &gt; ssthresh，使用拥塞控制算法，停用慢启动算法；当cwnd &#x3D; ssthresh，这两个算法都可以。拥塞控制算法具体为：每经历过一次往返时间就使cwnd增加1，使cwnd缓慢增长。</p></li><li><p><strong>快重传</strong>：接收方收到一个失序的报文段后就立刻发出理应发送的ACK报文。如果发送方连续收到三个重复的ACK，则发送方立刻重传相关报文段，而不必等待重传计时器到期。</p></li><li><p><strong>快恢复</strong>：当发送发连续接收到三个确认时，就把慢启动的阈值（ssthresh）减为当前cwnd的一半，然后将cwnd设置为ssthresh，继续拥塞避免算法。</p></li><li><p><strong>AIMD（additive-increase&#x2F;multiplicative-decrease）原则</strong>：加法增加，乘法减小。当网络频发出现超时情况时，为了减少注入到网络中的分组数，ssthresh就下降的很快；而加法增大是指执行拥塞避免算法后，拥塞窗口缓慢增大，以防止网络过早出现拥塞。</p></li></ul><p>经典的示意图如下图所示：</p><img src="/2022/04/23/TCP/RENO.png" class=""><p>注意：滑动窗口的大小实际上是由两个窗口共同决定的，一个是接收端的通告窗口，另一个是拥塞窗口，滑动窗口的大小就是通告窗口和拥塞窗口的较小值。通告窗口的窗口值在TCP协议头部信息中有，会随着数据的ACK包发送给发送端，这个值表示的是在接收端的TCP缓存中还有多少剩余空间，发送端必须保证发送的数据不超过这个剩余空间，这个窗口是接收端用来进行流量限制的。另一个窗口是发送端的拥塞窗口(Congestion window)，由发送端维护这个值，在协议头部信息中没有，所以拥塞窗口可以看作是发送端用来进行流量控制的窗口。</p><h2 id="BIC算法（Binary-Increase-Congestion）"><a href="#BIC算法（Binary-Increase-Congestion）" class="headerlink" title="BIC算法（Binary Increase Congestion）"></a>BIC算法（Binary Increase Congestion）</h2><blockquote><p>参考：<a href="https://blog.csdn.net/dog250/article/details/53013410">https://blog.csdn.net/dog250/article/details/53013410</a></p><p><a href="https://blog.csdn.net/zk3326312/article/details/91375314">https://blog.csdn.net/zk3326312/article/details/91375314</a></p></blockquote><p>传统的TCP算法，例如TCP-Reno等在<strong>长肥网络</strong>下不能充分利用网络带宽，原因在于在进入拥塞避免阶段后，它们的拥塞窗口每经过一个RTT才加1，拥塞窗口的增长速度太慢，当碰上高带宽环境时，可能需要经历很多个RTT，拥塞窗口才能接近于一个BDP。如果数据流很短，可能拥塞窗口还没增长到一个BDP，数据流就已经结束了，这种情况的带宽利用率就会非常低。</p><p>BIC算法通过二分查找方法来设置拥塞窗口，现在假设“窗口最大值”代表“能将线路满载但不丢包”，则有以下的事实：</p><ol><li><p>假设发生丢包时拥塞窗口的大小是W1，那么若要保持线路满载却不丢包，实际的窗口最大值应该在W1以下；</p></li><li><p>如果检测到发生丢包，并且已经将窗口乘性减到了W2，那么实际的窗口最大值应该在W2以上。</p></li></ol><p>因此，在TCP快速恢复阶段后，便开始在W2~W1这个区间内进行二分搜索，寻找窗口的实际最大值。定义W1为Wmax，定义W2为Wmin，发送方每收到一个ACK的时候，便将窗口设置到Wmax和Wmin的中点，一直持续到接近Wmax。也就是说，发送方的拥塞窗口的增长是和RTT有关，如果画一个坐标轴，竖轴代表发送方的拥塞窗口大小，则横轴则以RTT为一个时间单位，而非以绝对时间为单位。</p><p>下图是经典的BIC算法的增长曲线，竖轴代表拥塞窗口大小，横轴可以理解为以RTT为一个时间单位。</p><p>以竖着的虚线为分界线，先看虚线左边，加法增窗(Additive Increase)可以看到，二分搜索的过程，整个增窗流程趋近一个凸函数的左半边，这样带来的好处是：越接近Wmax附近，窗口的增加速度越慢，这也就意味着，在一次经历了一次丢包后，窗口会更快的接近W，并在W附近停留更多的时间。</p><p>再看虚线右边的图像，这个过程被称作Max Probing，即在探测当前合适的最大窗口。BIC算法的设计者认为，当窗口值超过Wmax以后，如果还未发生丢包，则说明网络变好了，或者有部分链接让出了资源，那么我们要尽可能的去抢占他，首先我们先慢慢的尝试，然后越来越快，以保证整个网络资源的利用率，因此Max Probing被设计为虚线左边的旋转对称的模样。</p><img src="/2022/04/23/TCP/BIC_1.png" class=""><p>由于BIC算法的拥塞窗口大小与RTT时间单位有关，会出现以下问题：如果两个连接RTT不同，则两者搜索到Wmax所需要的时间是不同的，进入到Max-Probe阶段所需要的时间也是不同的，因此空闲的带宽会被RTT短的那个连接先占有，不是很公平，如下图所示：</p><img src="/2022/04/23/TCP/BIC_2.png" class=""><h2 id="Cubic算法"><a href="#Cubic算法" class="headerlink" title="Cubic算法"></a>Cubic算法</h2><blockquote><p>参考：<a href="https://blog.csdn.net/zk3326312/article/details/91375314">https://blog.csdn.net/zk3326312/article/details/91375314</a></p><p><a href="https://blog.csdn.net/dog250/article/details/53013410">https://blog.csdn.net/dog250/article/details/53013410</a></p></blockquote><p>CUBIC的拥塞控制窗口增长函数是一个三次函数，曲线类似于BIC，但CUBIC的拥塞窗口增长独立于RTT，也就是说曲线的纵坐标是窗口值，横坐标为绝对时间，因此能更好的保证流与流之间的公平性。</p><h1 id="Loss-based可能产生的几个问题"><a href="#Loss-based可能产生的几个问题" class="headerlink" title="Loss-based可能产生的几个问题"></a>Loss-based可能产生的几个问题</h1><blockquote><p>参考：<a href="https://blog.lqs1848.top/2021/06/07/2021-06-07%20%20BBR/">https://blog.lqs1848.top/2021/06/07/2021-06-07%20%20BBR/</a></p></blockquote><ul><li><p>丢包即拥塞。现实中网络环境很复杂，会存在错误丢包，Loss-based算法无法很好区分拥塞丢包和错误丢包。产生错误丢包可能导致TCP速率断崖式下降，这在某些网络场景中并不能充分利用带宽。</p></li><li><p>BufferBloat：路由器缓冲区存放数据本身代表路由器转发速率小于接收速率，实际上在一定程度上代表了网络拥塞，但TCP的发送方并不能实时感知到拥塞的发生，因此TCP一直增长发送速率再次加剧了网络拥塞。</p></li><li><p>低负载高延时丢包：在某些弱网环境下RTT会增加甚至出现非拥塞引起丢包（固定丢包），此时基于丢包反馈的拥塞算法的窗口会比较小，对带宽的利用率很低，吞吐量下降很明显，但是实际上网络负载并不高，所以在弱网环境下效果并不是非常理想。</p></li><li><p>高负载丢包：高负载无丢包情况下算法一直加窗，这样可以预测丢包事件可能很快就出现了，一旦丢包出现窗口将呈现乘性减少，由高位发送速率迅速降低会造成整个网络的瞬时抖动性，总体呈现较大的锯齿状波动。</p></li></ul><h1 id="Buffer-bloat"><a href="#Buffer-bloat" class="headerlink" title="Buffer bloat"></a>Buffer bloat</h1><p>路由器的缓存大小也会影响到TCP的拥塞控制。</p><p>如果路由器的报文缓冲区太小，将导致丢包率高，数据链路利用率低，TCP传输效率低。具体来讲，当有新报文到达的时候，如果前面一个报文正在发送，这个报文缓冲区尚未处于空闲状态，那么新的报文势必将会被丢掉。等前面一个报文发送完了，链路处于空闲状态，但是由于刚才报文已经被丢掉了，也无法利用链路空闲状态。如果被丢掉的报文是TCP报文，可能导致发送方缩小自己的发送窗口，降低了TCP连接的速率。</p><p>Buffer bloat：如果路由器的缓冲区过大，大型的缓冲区使数据包不易被丢弃，数据包在队列中等待，这对Loss-based拥塞控制算法造成巨大的问题。在这种情况下，TCP 传送端并不知道拥塞的发生，仍持续增长传输速率。拥塞的信号(一个数据包丢失)需经很长时间才能反馈到发送端,此时在发送端和接收端之间缓存了大量数据，造成问题。</p><p>Q：如果网络中路由器的缓冲队列开始出现排队但没有丢包，TCP发送端的滑动窗口是变大还是变小？TCP的发送速率是变大还是变小？这条链路的吞吐量是变大还是变小？</p><p>A：没有丢包，滑动窗口会变大，TCP的发送速率会变大，但这条链路的吞吐量会保持不变。吞吐量：单位时间内成功传输的数据量。尽管TCP的发送速率变大，但路由器的转发速率有限，因此收到路由器转发速率的限制，吞吐量不会变大，会保持不变。除此之外，RTT也开始变大。</p><h1 id="Google-BBR算法（Bottleneck-Bandwidth-and-Round-trip）"><a href="#Google-BBR算法（Bottleneck-Bandwidth-and-Round-trip）" class="headerlink" title="Google BBR算法（Bottleneck Bandwidth and Round-trip）"></a>Google BBR算法（Bottleneck Bandwidth and Round-trip）</h1><blockquote><p>参考：<a href="http://arthurchiao.art/blog/bbr-paper-zh/">http://arthurchiao.art/blog/bbr-paper-zh/</a> （论文译文）</p><p><a href="https://switch-router.gitee.io/blog/bbr1/">https://switch-router.gitee.io/blog/bbr1/</a></p></blockquote><p>为了解决Loss-based可能产生的问题而诞生。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>在现代基础设施中， 丢包和延迟不一定表示网络发生了拥塞，因此原来的假设已经不再成立。 Google 的网络团队从这一根本问题出发，（在前人工作的基础上） 设计并实现了一个基于拥塞本身而非基于丢包或延迟的拥塞控制新算法，缩写为 BBR。BBR 通过应答包（ACK）中的 RTT 信息和已发送字节数来计算真实传输速率（delivery rate），然后根据后者来调节客户端接下来的发送速率（sending rate），通过保持合理的在途数据包数量来使使得传输速率最大、传输延迟最低。另外，它完全运行在发送端，无需协议、 接收端或网络的改动，因此落地相对容易。</p><h2 id="术语介绍"><a href="#术语介绍" class="headerlink" title="术语介绍"></a>术语介绍</h2><ul><li><p><strong>RTprop</strong>(route-trip propagation time)：RTT时间</p></li><li><p><strong>BtlBw</strong>(bottleneck bandwidth)：当前链路传输速率delivery rate的上限 （带宽）</p></li><li><p><strong>BDP</strong>(bandwidth-delay product) &#x3D; <strong>RTprop * BtlBw （带宽时延乘积）</strong></p></li></ul><h2 id="网络传输最优点"><a href="#网络传输最优点" class="headerlink" title="网络传输最优点"></a>网络传输最优点</h2><p>网络传输最优点：瓶颈链路被100%利用且未产生缓存队列，即**max(BtlBw)*min(RTprop)**。BBR算法的目标则是保持处于这个位。</p><img src="/2022/04/23/TCP/BBR_1.png" class=""><p>如图所示，该图展示了RTT和数据传输速率（delivery rate)随着在途报文的数量（amount in flight，即已发送但暂未收到回复的报文的数量)的增大而变化的情况。</p><p>该图通过两调竖着的虚线，可以分为三个区域：</p><ul><li><p>左侧区域：数据包不多，暂未填满整个链路管道。在这个区域内，数据传输速率会增加，但RTT不会增加。</p></li><li><p>中部区域：超出BDP的数据包会开始占用通信链路中的路由器buffer，产生队列，此时RTT会增加。</p></li><li><p>右侧区域：数据包数量继续增加，buffer被填满出现丢包现象。Loss-based拥塞控制算法开始在此起作用(如发送窗口减半再线性增加等)。</p></li></ul><p>我们可以看出，最小RTT和最大传输速率不能同时进行测量。如果我们要测量最大传输速率，则需要把链路填满，但这有可能产生队列，rtt可能较高；如果我们要测量最小RTT，则数据包越少越好，这导致传输速率较小。</p><p>因此BBR的解决办法是交替测量带宽和rtt，用一段时间内的带宽最大值和rtt最小值作为估计值。</p><h2 id="BBR算法介绍"><a href="#BBR算法介绍" class="headerlink" title="BBR算法介绍"></a>BBR算法介绍</h2><h3 id="Startup-amp-Drain"><a href="#Startup-amp-Drain" class="headerlink" title="Startup &amp; Drain"></a>Startup &amp; Drain</h3><p>Startup 是 BBR 控制的流启动时的状态，为了能尽快找到链路的瓶颈带宽 BtlBw，处于 Startup 状态的流每一个 RTT 会将报文发送速率会提高一倍。指数增长的发送速率使得amount in flight快速增加，也使得传输速率快速增加，从而 BBR 计算得到的 bandwith 也快速增加，当delivery rate 不再变化，此时 BBR 就能测量到最大传输速率（即bandwidth)。当 BBR 看到测量到的 bandwidth 在连续 3 个 RTT内不再显著增长时(增长幅度小于 25%)，变会退出 Startup 状态，进入 Drain 状态。</p><img src="/2022/04/23/TCP/BBR_STARTUP.png" class=""><p>Drain 状态的目标是让amount in flight回到 BDP 的水平，如图所示，使其回到转折点。该阶段存在的意义是为了抵消掉 Startup 状态后期向网络灌入的超量报文。随后，将进入 Probe Bandwidth 状态。</p><img src="/2022/04/23/TCP/BBR_DRAIN.png" class=""><h3 id="Probe-Bandwidth-状态"><a href="#Probe-Bandwidth-状态" class="headerlink" title="Probe Bandwidth 状态"></a>Probe Bandwidth 状态</h3><p>Probe Bandwidth 是四个状态中唯一的稳态，也是持续时间最长的状态。在此状态下，BBR 会不断的去探测(或者说是压榨)带宽。</p><p>BBR 定义了一个gain cycling的概念，以此控制报文发送速度。具体来说，一个 cycle 包含 8 个阶段，每个阶段的持续时间为 1 个 min RTT。8 个阶段的增益系数分别为：1.25、0.75、1、1、1、1、1、1。当处于增益系数值为 1.25 的阶段时，意味着发送速率是当前计算值的 1.25 倍，此时 BBR 发送速率也会变成正常情况下的 1.25 倍(踩油门)。而当处于增益系数值为 0.75 的阶段时，相应的发送速率是正常情况的 0.75 倍(踩刹车)。而增益系数值为 1 时，发送速率就是正常值(巡航)。</p><p>BBR 一脚油门一脚刹车的组合保证了当链路带宽不变时，这条流不会向链路灌入超量的报文。而 6&#x2F;8 时间段里的增益系数值为 1 又使得大部分时候，发送报文的速率是稳定的。</p><img src="/2022/04/23/TCP/BBR_PROBE_BW.png" class=""><h3 id="Probe-RTT-状态"><a href="#Probe-RTT-状态" class="headerlink" title="Probe RTT 状态"></a>Probe RTT 状态</h3><p>Probe RTT 状态的目的是为了探测链路的固有往返时延(RTprop)，如果 min-RTT 在 10s 内没有刷新，则无论目前BBR处于哪个状态，BBR 都会强制将状态切换到 Probe RTT。进入 Probe RTT 状态后，BBR 会将拥塞窗口限制到一个非常小的值，并持续至少 200ms, 目的是保证这条流不会引起中间设备上的报文堆积。</p><img src="/2022/04/23/TCP/BBR_PROBE_RTT.png" class=""><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>现在我们可以回答前言提出的几个问题：</p><ol><li>TCP-RENO算法通过实现AIMD（加性增乘性减）原则来控制拥塞窗口，其局限性在于进入拥塞避免阶段窗口增速太慢，在长肥网络的场景下不能充分利用网络带宽。</li><li>为了解决带宽利用问题，TCP-BIC和CUBIC算法在拥塞避免阶段将窗口大小的增加速度进行提速。</li><li>BBR算法主要是为了解决buffer bloat的情况。TCP传统拥塞算法感知不到buffer bloat的状态，会持续向链路中发包，数据传输速率不会因此增加，而链路反而会变得更加拥塞。BBR算法通过估计带宽和RTT，尽可能使链路维持在带宽利用充分但不排队的状态，以此来维持最大数据传输速率。和TCP拥塞算法相比，BBR算法主要基于带宽和RTT的估计来运作。</li></ol>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;发生拥塞控制的原因：资源(带宽、交换节点的缓存、处理机)的需求&amp;gt;可用资源。拥塞控制就是为了防止过多的数据注入到网络中，这样可以使网络中的路由器或者链路不至于过载。&lt;/p&gt;
&lt;p&gt;本篇学习笔记需要理清的问题有以下几点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;TCP RENO算法（教科书算法）是如何控制网络拥塞的？有什么局限性？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;为了解决这个局限性，TCP的拥塞算法是如何改进的？&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Google提出的BBR算法和TCP广泛使用的拥塞算法相比，有何不同？有何优点？&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="计算机网络" scheme="http://example.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"/>
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://example.com/2022/04/23/hello-world/"/>
    <id>http://example.com/2022/04/23/hello-world/</id>
    <published>2022-04-23T12:26:53.000Z</published>
    <updated>2022-06-05T09:09:53.532Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hello-world"><a href="#Hello-world" class="headerlink" title="Hello world"></a>Hello world</h1><p>以前觉得写博客没有什么用，因为很多知识点都能通过搜索引擎搜到。现在发现这样的观点会带来一个弊端：搜索得到的内容并未被我完全吸收，用过就忘。除此之外，现在网上博客繁多，鱼龙混杂，需要一定的知识基础才能分辨出何为好何为坏，而分辨也需要一定的时间和精力。</p><p>写博客就像写读书笔记一样，可以使人对内容理解更深。古人云：眼过千遍，不如手写一遍。很多时候我们读了一些书，却不做笔记或者写感想，久而久之，不要说是书的内容了，读书的情绪和感受大概率也忘得一干二净。本来是花费了时间，到头来却毫无收获，这种滋味真的不太好受。更不要说职业生涯都需要用到的专业知识技能了，如此反反复复炒冷饭，学过就忘，忘了再学，总觉得是对时间的一种浪费。我们从记忆理论可以知道，长期记忆其实也并不难，也并不需要花费特别多的时间。“勿以善小而不为”，对于所学的知识，不管是专业知识还是非专业知识，我都希望将其形成长期记忆。我相信当人的知识积累越来越丰厚时，才可能有能力发现“新大陆”，而无知识积累的人即使发现了“新大陆”，也会将其视为“破砖烂瓦”，而体会不到新事物的乐趣。</p><p>以此文为博客的开头，时刻提醒我自己。虽然我要克服我的缺点（这里指懒惰）不是一件简单的事情，但这是一件有价值的事情。通过我看小说和漫画的颇为贫乏的经验来看，固步自封、畏葸不前以及遇到困难就退缩的人不可能成为生活的主角，主角多半会具有冒险精神和不畏困难的精神，不然故事怎么继续下去呢？</p><p>集腋成裘。努力做自己生活的主角吧。</p><p>最后：talk is cheap。希望自己能知行合一，做一个善用头脑的行动派。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hello-world&quot;&gt;&lt;a href=&quot;#Hello-world&quot; class=&quot;headerlink&quot; title=&quot;Hello world&quot;&gt;&lt;/a&gt;Hello world&lt;/h1&gt;&lt;p&gt;以前觉得写博客没有什么用，因为很多知识点都能通过搜索引擎搜到。现在发</summary>
      
    
    
    
    
  </entry>
  
</feed>
